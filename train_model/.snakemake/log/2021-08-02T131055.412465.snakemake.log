Building DAG of jobs...
Using shell: /usr/bin/bash
Provided cores: 1 (use --cores to define parallelism)
Rules claiming more threads will be scaled down.
Job counts:
	count	jobs
	1	all
	1	sample_genome_for_training
	2

[Mon Aug  2 13:10:56 2021]
rule sample_genome_for_training:
    input: ../../data/hg19.chrom-sizes
    output: ../../pyro_model/K562_hg19/train_data/frac_0.1/sample_genome.bed.gz
    jobid: 129
    wildcards: train_frac=0.1

[Mon Aug  2 13:10:56 2021]
Error in rule sample_genome_for_training:
    jobid: 129
    output: ../../pyro_model/K562_hg19/train_data/frac_0.1/sample_genome.bed.gz
    shell:
        
		python sample_genome_for_training.py ../../data/hg19.chrom-sizes 0.1 ../../pyro_model/K562_hg19/train_data/frac_0.1/sample_genome.bed.gz
		
        (one of the commands exited with non-zero exit code; note that snakemake uses bash strict mode!)

Shutting down, this might take some time.
Exiting because a job execution failed. Look above for error message
Complete log: /gstore/home/vuh6/source/train_model/.snakemake/log/2021-08-02T131055.412465.snakemake.log
