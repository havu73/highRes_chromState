{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0dd55cad-73e1-4762-b687-59cb1bd5040c",
   "metadata": {},
   "source": [
    "# ChromStateTransferVAE\n",
    "\n",
    "An attempt at getting to a variational encoder to annotate chromatin state in samples of interest with small number of signals by using\n",
    "state annotation from state annotations from reference epigenomes\n",
    "\n",
    "## The problem statement\n",
    "\n",
    "The data for the VAE will consist of:\n",
    "\n",
    "- $m$ is a signal matrix of size $L$ genomic positions by $N$ number of observed signals\n",
    "- $r$ is a reference epigenome state annotation indicator matrix of size $L$, $R$ (number of references) $S$ number of states.\n",
    "\n",
    "The goal is to generate a state annotation (from set $S$) for signal matrix $m$\n",
    "\n",
    "## Generating simulated data\n",
    "\n",
    "To generate data we will use the following generative model. Simulation parameters are:\n",
    "\n",
    "- $\\alpha$ prior over reference\n",
    "- $r$ reference state indicator matrix\n",
    "- $p$ state to signal parameter matrix\n",
    "\n",
    "with those, pseudo code for the generative model for a specific genomic location is given by\n",
    "\n",
    "```\n",
    "1. generate probability distribution over references \n",
    "   pi (shape: (num_references,)) ~ Dirichlet(alpha)\n",
    "2. collapse pi to a probability distribution over states \n",
    "   collapsed_pi (shape: (num_states,)) = r * pi (i don't think these dimensions match)\n",
    "3. generate sample state z (shape (1,)) ~ Categorical(collapsed_pi)\n",
    "4. generate signal vector m (shape: (num_signals,)) ~ Bernoulli(p[z,:])\n",
    "```\n",
    "\n",
    "### Prior over reference\n",
    "\n",
    "$\\alpha$ a vector of size $R$ used as prior parameter of a Categorical distribution of references, may be interpreted with how similar the sample of\n",
    "interest is to each reference. Here are some useful cases:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b32c3f6d-6cb1-430f-83cd-fe8c7fffae8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# case 1: sample of interest is essentially identical to one of the references\n",
    "def generate_single_reference_alpha(num_references, w=10):\n",
    "    alpha = tensor.ones((num_references))\n",
    "    alpha[0] = w\n",
    "    return alpha\n",
    "\n",
    "# case 2: sample of interest is equally similar to a small number of the references\n",
    "def generate_batch_reference_alpha(num_references, batch_size, w=10):\n",
    "    alpha = tensor.ones((num_references))\n",
    "    alpha[0:batch_size] = w\n",
    "    return alpha\n",
    "\n",
    "# note case 1 is a special case of case 2, so only a single function needed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "806d811e-d6d6-4770-8c57-96e4be294307",
   "metadata": {},
   "source": [
    "### Reference state indicator matrix\n",
    "\n",
    "This encodes the state annotation along the genome for each of the references"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "57739a80-a5aa-4eed-a8a5-f25b99e89fc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# helper function to create an indicator matrix from an assignment matrix\n",
    "def generate_indicator(assignments, num_states):\n",
    "    return F.one_hot(assignments.long(), num_classes=num_states)\n",
    "\n",
    "# case 1: there is a single reference that matters\n",
    "def generate_single_reference_r(num_positions, num_references, num_states, state_sequence_length=3):\n",
    "    assignments = torch.zeros(num_positions, num_references)\n",
    "    # the important refernces has a unique state sequence\n",
    "    state_sequence = torch.arange(state_sequence_length)\n",
    "    num_times = math.ceil(num_positions / len(state_sequence))\n",
    "    state_sequence = state_sequence.repeat(num_times)\n",
    "    assignments[:,0] = state_sequence[:num_positions]\n",
    "\n",
    "    random_assignments = torch.multinomial(\n",
    "            torch.ones(num_states), \n",
    "            num_samples=num_positions*(num_references - 1),\n",
    "            replacement=True)\\\n",
    "        .reshape((num_positions,-1))\n",
    "        \n",
    "    assignments[:,1:] = random_assignments\n",
    "    return generate_indicator(assignments, num_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "97f7f59f-b4e8-42ad-b51f-50e69034a0b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[1, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 1, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 1, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 1, 0, 0],\n",
       "         [0, 0, 0, 0, 1, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 0, 1, 0],\n",
       "         [0, 0, 0, 0, 0, 0, 0, 1],\n",
       "         [0, 0, 0, 0, 0, 1, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 1, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 0, 0, 1]],\n",
       "\n",
       "        [[0, 1, 0, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 0, 0, 1],\n",
       "         [0, 0, 0, 0, 1, 0, 0, 0],\n",
       "         [0, 0, 1, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 0, 0, 1],\n",
       "         [0, 0, 0, 0, 0, 1, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 0, 0, 1],\n",
       "         [0, 0, 0, 1, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 1, 0, 0, 0, 0],\n",
       "         [1, 0, 0, 0, 0, 0, 0, 0]],\n",
       "\n",
       "        [[0, 0, 1, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 1, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 1, 0, 0, 0],\n",
       "         [1, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 0, 0, 1],\n",
       "         [0, 0, 0, 0, 0, 1, 0, 0],\n",
       "         [0, 1, 0, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 1, 0, 0, 0],\n",
       "         [0, 1, 0, 0, 0, 0, 0, 0],\n",
       "         [1, 0, 0, 0, 0, 0, 0, 0]]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_single_reference_r(20, 10, 8)[:3,:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "d0304050-79b4-44bc-a89d-536df7e4f141",
   "metadata": {},
   "outputs": [],
   "source": [
    "# case 2: there is a batch of similar references, but, their state sequences\n",
    "# are identical\n",
    "def generate_batch_identical_references_r(\n",
    "        num_positions, \n",
    "        num_references, \n",
    "        num_states, \n",
    "        batch_size, \n",
    "        state_sequence_length=3):\n",
    "    assignments = torch.zeros(num_positions, num_references)\n",
    "    # the important references has a unique state sequence\n",
    "    num_times = math.ceil(num_positions / state_sequence_length)\n",
    "    state_sequence = torch.arange(state_sequence_length)\\\n",
    "        .repeat_interleave(batch_size)\\\n",
    "        .reshape(-1,batch_size)\\\n",
    "        .repeat((num_times,1))[:num_positions,:]\n",
    "    assignments[:,0:batch_size] = state_sequence\n",
    "    random_assignments = torch.multinomial(\n",
    "            torch.ones(num_states), \n",
    "            num_samples=num_positions*(num_references - batch_size),\n",
    "            replacement=True)\\\n",
    "        .reshape((num_positions,-1))\n",
    "        \n",
    "    assignments[:,batch_size:] = random_assignments\n",
    "    return generate_indicator(assignments, num_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "ae0e01a3-3f87-4581-bb69-ddfc19a45211",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[1, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [1, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [1, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 1, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 1, 0, 0],\n",
       "         [0, 0, 1, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 1, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 1, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 1, 0, 0, 0],\n",
       "         [0, 1, 0, 0, 0, 0, 0, 0]],\n",
       "\n",
       "        [[0, 1, 0, 0, 0, 0, 0, 0],\n",
       "         [0, 1, 0, 0, 0, 0, 0, 0],\n",
       "         [0, 1, 0, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 1, 0, 0],\n",
       "         [0, 0, 1, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 1, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 0, 1, 0],\n",
       "         [0, 0, 0, 0, 0, 0, 0, 1],\n",
       "         [0, 0, 0, 1, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 0, 0, 1]],\n",
       "\n",
       "        [[0, 0, 1, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 1, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 1, 0, 0, 0, 0, 0],\n",
       "         [1, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [1, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 1, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 1, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 1, 0, 0],\n",
       "         [0, 0, 0, 1, 0, 0, 0, 0],\n",
       "         [1, 0, 0, 0, 0, 0, 0, 0]]])"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_batch_identical_references_r(20,10,8,3)[:3,:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "11259148-bbb6-49c1-824a-233541565b13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# case 3: there is a batch of references the sample is similar to\n",
    "# but their state sequences are different (this is where it's a mixture)\n",
    "def generate_batch_different_references_r(\n",
    "        num_positions,\n",
    "        num_references,\n",
    "        num_states,\n",
    "        batch_size,\n",
    "        state_sequence_length=3):\n",
    "    assignments = torch.zeros(num_positions, num_references)\n",
    "    num_times = math.ceil(num_positions / state_sequence_length)\n",
    "    state_sequence = torch.arange(state_sequence_length)\\\n",
    "        .repeat_interleave(batch_size)\\\n",
    "        .reshape(-1, batch_size)\\\n",
    "        .add(torch.arange(batch_size).repeat(state_sequence_length,1))\\\n",
    "        .repeat((num_times,1))[:num_positions,:]\n",
    "    assignments[:,0:batch_size] = state_sequence\n",
    "    random_assignments = torch.multinomial(\n",
    "        torch.ones(num_states),\n",
    "        num_samples=num_positions * (num_references - batch_size),\n",
    "        replacement=True)\\\n",
    "    .reshape((num_positions,-1))\n",
    "    \n",
    "    assignments[:,batch_size:] = random_assignments\n",
    "    return generate_indicator(assignments, num_states)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "73aea2a9-7931-4745-8edf-a3861b63e25c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[1, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [0, 1, 0, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 1, 0, 0, 0, 0, 0],\n",
       "         [0, 1, 0, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 0, 0, 1],\n",
       "         [0, 0, 1, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 1, 0, 0],\n",
       "         [1, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 1, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 0, 1, 0]],\n",
       "\n",
       "        [[0, 1, 0, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 1, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 1, 0, 0, 0, 0],\n",
       "         [0, 0, 1, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 1, 0, 0, 0],\n",
       "         [0, 0, 1, 0, 0, 0, 0, 0],\n",
       "         [1, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 0, 1, 0],\n",
       "         [0, 0, 0, 0, 0, 0, 0, 1],\n",
       "         [0, 0, 0, 0, 0, 1, 0, 0]],\n",
       "\n",
       "        [[0, 0, 1, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 1, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 1, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 0, 0, 1],\n",
       "         [0, 0, 1, 0, 0, 0, 0, 0],\n",
       "         [1, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 0, 0, 1],\n",
       "         [1, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 0, 1, 0],\n",
       "         [0, 1, 0, 0, 0, 0, 0, 0]]])"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_batch_different_references_r(20,10,8,3)[:3,:,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1490940-1961-4c36-9961-230f73462179",
   "metadata": {},
   "source": [
    "### state to signal parameter matrix\n",
    "\n",
    "This is a matrix of shape $S$\\times$N$ (number of states by number of signals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "eadcc2e6-7126-4f28-bd9f-76c9a2550c61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper function to turn a state number to a signal pattern\n",
    "def state_to_signal_pattern(state, num_signals):\n",
    "    return torch.tensor([int(x) for x in list('{0:03b}'.format(state+1).zfill(num_signals))])\n",
    "\n",
    "# case 1: the \"important states\" are perfectly specified by the signal pattern\n",
    "# signal for other states is random (0.5)\n",
    "def generate_specific_signal_p(\n",
    "        num_states,\n",
    "        num_signals,\n",
    "        num_important_states,\n",
    "        weight=10):\n",
    "    assert num_important_states < 2**num_signals\n",
    "    p = 0.5 * torch.ones((num_states,num_signals))\n",
    "    for i in range(num_important_states):\n",
    "        p[i,:] = state_to_signal_pattern(i, num_signals)\n",
    "    p = weight * (2*p - 1)\n",
    "    return torch.sigmoid(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "896f8cb1-4edc-4767-b4fe-257be503d0a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[4.5398e-05, 4.5398e-05, 9.9995e-01],\n",
       "        [4.5398e-05, 9.9995e-01, 4.5398e-05],\n",
       "        [4.5398e-05, 9.9995e-01, 9.9995e-01],\n",
       "        [9.9995e-01, 4.5398e-05, 4.5398e-05],\n",
       "        [5.0000e-01, 5.0000e-01, 5.0000e-01],\n",
       "        [5.0000e-01, 5.0000e-01, 5.0000e-01],\n",
       "        [5.0000e-01, 5.0000e-01, 5.0000e-01],\n",
       "        [5.0000e-01, 5.0000e-01, 5.0000e-01],\n",
       "        [5.0000e-01, 5.0000e-01, 5.0000e-01],\n",
       "        [5.0000e-01, 5.0000e-01, 5.0000e-01],\n",
       "        [5.0000e-01, 5.0000e-01, 5.0000e-01],\n",
       "        [5.0000e-01, 5.0000e-01, 5.0000e-01]])"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_specific_signal_p(12,3,4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "8a04ff8c-ab96-4429-9f11-59bd315bf134",
   "metadata": {},
   "outputs": [],
   "source": [
    "# case 2: the \"similar states\" are perfectly specified by the signal\n",
    "# pattern, but one other \"non-similar state\" shares the same\n",
    "# signal pattern\n",
    "def generate_similar_signal_p(\n",
    "        num_states,\n",
    "        num_signals,\n",
    "        num_important_states,\n",
    "        weight=10):\n",
    "    assert 2 * num_important_states < 2**num_signals\n",
    "    p = 0.5 * torch.ones((num_states, num_signals))\n",
    "    for i in range(num_important_states):\n",
    "        p[i,:] = state_to_signal_pattern(i, num_signals)\n",
    "        p[i+num_important_states,:] = p[i,:]\n",
    "    p = weight * (2*p - 1)\n",
    "    return torch.sigmoid(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "581a421d-13a9-4842-8e9a-b457691817a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[4.5398e-05, 4.5398e-05, 9.9995e-01],\n",
       "        [4.5398e-05, 9.9995e-01, 4.5398e-05],\n",
       "        [4.5398e-05, 9.9995e-01, 9.9995e-01],\n",
       "        [4.5398e-05, 4.5398e-05, 9.9995e-01],\n",
       "        [4.5398e-05, 9.9995e-01, 4.5398e-05],\n",
       "        [4.5398e-05, 9.9995e-01, 9.9995e-01],\n",
       "        [5.0000e-01, 5.0000e-01, 5.0000e-01],\n",
       "        [5.0000e-01, 5.0000e-01, 5.0000e-01],\n",
       "        [5.0000e-01, 5.0000e-01, 5.0000e-01],\n",
       "        [5.0000e-01, 5.0000e-01, 5.0000e-01],\n",
       "        [5.0000e-01, 5.0000e-01, 5.0000e-01],\n",
       "        [5.0000e-01, 5.0000e-01, 5.0000e-01]])"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_similar_signal_p(12,3,3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06f6afd1-a43c-47e5-849e-2730b32dc6fe",
   "metadata": {},
   "source": [
    "Ok, let's refactor some of these and create classes to encapsulate all of these behaviors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "3165328a-52e0-4c02-9498-456c76008282",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AlphaGenerator:\n",
    "    def __init__(self, num_references, w=10):\n",
    "        self.num_references = num_references\n",
    "        self.w = w\n",
    "        \n",
    "    def init_alpha(self):\n",
    "        return torch.ones((self.num_references))\n",
    "    \n",
    "    def set_alpha(self, alpha):\n",
    "        alpha[0] = self.w\n",
    "        return alpha\n",
    "    \n",
    "    def generate(self):\n",
    "        return self.update_alpha(self.init_alpha())\n",
    "    \n",
    "    def update_alpha(self, alpha):\n",
    "        return alpha\n",
    "    \n",
    "class SingleAlphaGenerator(AlphaGenerator):\n",
    "    def __init__(self, num_references, w=10):\n",
    "        super().__init__(num_references, w)\n",
    "        \n",
    "class BatchAlphaGenerator(AlphaGenerator):\n",
    "    def __init__(self, num_references, batch_size, w=10):\n",
    "        super().__init__(num_references, w)\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "    def update_alpha(self, alpha):\n",
    "        alpha[:self.batch_size] = w\n",
    "        return alpha\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "b2331c9c-2270-442e-b5fc-4f234623e53b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])\n"
     ]
    }
   ],
   "source": [
    "print(SingleAlphaGenerator(12).generate())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f50a323-8925-44a3-bea8-3d4506498a03",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
