{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "142a3e53-b19f-4879-a6cb-825f62f50bcf",
   "metadata": {},
   "source": [
    "## Replicate the prodlda pyro notebook\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "45365249-1579-4d2d-bbd4-0f5b10a5cd8d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.7.0'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import pyro\n",
    "import pyro.distributions as dist\n",
    "from pyro.infer import MCMC, NUTS\n",
    "import torch\n",
    "\n",
    "pyro.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "21e3f292-3cd3-4831-ad73-656ea1584c12",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 2.,  1.,  9.,  1., 14.,  3.])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def model(counts):\n",
    "    theta = pyro.sample('theta', dist.Dirichlet(torch.ones(6)))\n",
    "    total_count = int(counts.sum())\n",
    "    pyro.sample('counts', dist.Multinomial(total_count, theta), obs=counts)\n",
    "    \n",
    "data = torch.tensor([5, 4, 2, 5, 6, 5, 3, 3, 1, 5, 5, 3, 5, 3, 5, \\\n",
    "                     3, 5, 5, 3, 5, 5, 3, 1, 5, 3, 3, 6, 5, 5, 6])\n",
    "counts = torch.unique(data, return_counts=True)[1].float()\n",
    "counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "28e19e50-56e3-496d-98c1-c080cd81c7a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sample: 100%|███████████████████████████████████████| 1200/1200 [00:10, 118.61it/s, step size=6.99e-01, acc. prob=0.925]             \n"
     ]
    }
   ],
   "source": [
    "nuts_kernel = NUTS(model)\n",
    "num_samples, warmup_steps = (1000, 200)\n",
    "mcmc = MCMC(nuts_kernel, num_samples=num_samples, warmup_steps=warmup_steps)\n",
    "mcmc.run(counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1407cf0a-40d9-4085-8096-c8c1daddca3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.12436687 0.01852442 0.32760313 0.03508684 0.3575466  0.13687217]\n",
      " [0.0805072  0.09267426 0.19635084 0.09863936 0.44795045 0.08387788]\n",
      " [0.09843706 0.02473691 0.41141734 0.0394576  0.25750944 0.16844165]\n",
      " ...\n",
      " [0.15093319 0.11145476 0.18145242 0.05172162 0.41899315 0.08544484]\n",
      " [0.04414841 0.01040715 0.41621354 0.0722887  0.39905074 0.0578914 ]\n",
      " [0.03533946 0.00421056 0.27795708 0.07612259 0.5142106  0.09215973]]\n"
     ]
    }
   ],
   "source": [
    "hmc_samples = {k: v.detach().cpu().numpy() for k, v, in mcmc.get_samples().items()}\n",
    "print (hmc_samples['theta'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "075af37e-1192-44e1-9dc1-e52c6b15b393",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1: 0.08 ± 0.04\n",
      "2: 0.06 ± 0.04\n",
      "3: 0.28 ± 0.07\n",
      "4: 0.05 ± 0.04\n",
      "5: 0.42 ± 0.08\n",
      "6: 0.11 ± 0.06\n"
     ]
    }
   ],
   "source": [
    "means = hmc_samples['theta'].mean(axis=0)\n",
    "stds = hmc_samples['theta'].std(axis=0)\n",
    "for i in range(6):\n",
    "    print('%d: %.2f \\u00B1 %.2f' % (i+1, means[i], stds[i]))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "789b9be1-1143-480d-a9bf-aeb2b6fab421",
   "metadata": {},
   "source": [
    "Now using categorical instead instead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e5c88dbb-7ef3-498d-af07-376c06140b30",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sample: 100%|███████████████████████████████████████| 1200/1200 [00:10, 109.39it/s, step size=5.76e-01, acc. prob=0.938]             \n"
     ]
    }
   ],
   "source": [
    "def model(data):\n",
    "    theta = pyro.sample('theta', dist.Dirichlet(torch.ones(6)))\n",
    "    with pyro.plate('data', len(data)):\n",
    "        pyro.sample('obs', dist.Categorical(theta), obs=data)\n",
    "    \n",
    "nuts_kernel = NUTS(model)\n",
    "num_samples, warmup_steps = (1000, 200)\n",
    "mcmc = MCMC(nuts_kernel, num_samples=num_samples, warmup_steps=warmup_steps)\n",
    "mcmc.run(data - 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "135f1420-ced9-4a9c-8672-e5ec0488ddff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1: 0.08 ± 0.04\n",
      "2: 0.05 ± 0.04\n",
      "3: 0.28 ± 0.08\n",
      "4: 0.06 ± 0.04\n",
      "5: 0.42 ± 0.08\n",
      "6: 0.11 ± 0.05\n"
     ]
    }
   ],
   "source": [
    "hmc_samples = {k: v.detach().cpu().numpy() for k, v in mcmc.get_samples().items()}\n",
    "means = hmc_samples['theta'].mean(axis=0)\n",
    "stds = hmc_samples['theta'].std(axis=0)\n",
    "for i in range(6):\n",
    "    print('%d: %.2f \\u00B1 %.2f' % (i+1, means[i], stds[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "834021f6-71e0-46cb-bb88-0f7311353f3a",
   "metadata": {},
   "source": [
    "the prodlda bit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8024dacd-823b-41c4-85e6-95ac36e04b27",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as mp\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cbdcd3cd-c1e7-45f3-b604-5b7e7d1125c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([18846, 12722])\n"
     ]
    }
   ],
   "source": [
    "news = fetch_20newsgroups(subset='all')\n",
    "vectorizer = CountVectorizer(max_df=0.5, min_df=20, stop_words='english')\n",
    "docs = torch.from_numpy(vectorizer.fit_transform(news['data']).toarray())\n",
    "print(docs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b9408401-660a-4f78-b5be-f4483c81dad6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         word  index\n",
      "0          00      0\n",
      "1         000      1\n",
      "2        0001      2\n",
      "3        0002      3\n",
      "4         001      4\n",
      "...       ...    ...\n",
      "12717    zoom  12717\n",
      "12718    zuma  12718\n",
      "12719  zurich  12719\n",
      "12720      zx  12720\n",
      "12721      zz  12721\n",
      "\n",
      "[12722 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "vocab = pd.DataFrame(columns=['word', 'index'])\n",
    "vocab['word'] = vectorizer.get_feature_names()\n",
    "vocab['index'] = vocab.index\n",
    "print(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d3daf59e-0898-4c5b-bc00-7b7448d883ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary size: 12722\n",
      "Corpus size: torch.Size([18846, 12722])\n"
     ]
    }
   ],
   "source": [
    "print('Dictionary size: %d' % len(vocab))\n",
    "print('Corpus size: {}'.format(docs.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "97de9ad5-9e2c-4af5-87b4-a1cafb4be825",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from pyro.infer import SVI, TraceMeanField_ELBO\n",
    "from tqdm import trange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "49cab5cd-1ae5-436d-b98c-13d847cad0be",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module): \n",
    "    # takes inputs: counts of word in a doc\n",
    "    # outputs: logtheta_loc and log_theta_scale, each of size=# topics \n",
    "    # encoder is used in the guide function\n",
    "    def __init__(self, vocab_size, num_topics, hidden, dropout):\n",
    "        super().__init__()\n",
    "        self.drop = nn.Dropout(dropout)\n",
    "        self.fc1 = nn.Linear(vocab_size, hidden) # layer 1 of linear transformation\n",
    "        self.fc2 = nn.Linear(hidden, hidden) # layer 2 of linear transformation\n",
    "        self.fcmu = nn.Linear(hidden, num_topics) # layer 3\n",
    "        self.fclv = nn.Linear(hidden, num_topics) # layer 4, actually, as far as I understand, #layers is our choice\n",
    "        self.bnmu = nn.BatchNorm1d(num_topics, affine=False)\n",
    "        self.bnlv = nn.BatchNorm1d(num_topics, affine=False)\n",
    "        \n",
    "    def forward(self, inputs): #inputs is a vector of size=# words in the dictionary\n",
    "        h = F.softplus(self.fc1(inputs))\n",
    "        h = F.softplus(self.fc2(h))\n",
    "        h = self.drop(h)\n",
    "        logtheta_loc = self.bnmu(self.fcmu(h))\n",
    "        logtheta_loc = logtheta_loc\n",
    "        logtheta_logvar = self.bnlv(self.fclv(h))\n",
    "        logtheta_scale = (0.5 * logtheta_logvar).exp()\n",
    "        return logtheta_loc, logtheta_scale\n",
    "    \n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, vocab_size, num_topics, dropout):\n",
    "        super().__init__()\n",
    "        self.beta = nn.Linear(num_topics, vocab_size, bias=False)\n",
    "        self.bn = nn.BatchNorm1d(vocab_size, affine=False)\n",
    "        self.drop = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        # inputs: theta (topic mixture parameters)\n",
    "        # outputs: vector of size=#words in dictionary, probabilities of observing a word \n",
    "        # in the paper, this prob. is (beta*theta)\n",
    "        inputs = self.drop(inputs)\n",
    "        return F.softmax(self.bn(self.beta(inputs)), dim=1)\n",
    "    \n",
    "class ProdLDA(nn.Module):\n",
    "    def __init__(self, vocab_size, num_topics, hidden, dropout):\n",
    "        super().__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.num_topics = num_topics\n",
    "        self.encoder = Encoder(vocab_size, num_topics, hidden, dropout)\n",
    "        self.decoder = Decoder(vocab_size, num_topics, dropout)\n",
    "        \n",
    "    def model(self, docs):\n",
    "        pyro.module('decoder', self.decoder)\n",
    "        # Dirichlet prior 𝑝(𝜃|𝛼) is replaced by a logistic-normal distribution\n",
    "        with pyro.plate('documents', docs.shape[0]):\n",
    "            # each doc gets a vector of theta (topic mixture)\n",
    "            logtheta_loc = docs.new_zeros((docs.shape[0], self.num_topics))\n",
    "            logtheta_scale = docs.new_ones((docs.shape[0], self.num_topics))\n",
    "            logtheta = pyro.sample(\n",
    "                'logtheta', dist.Normal(logtheta_loc, logtheta_scale).to_event(1))\n",
    "            theta = F.softmax(logtheta, -1)            \n",
    "            # conditional distribution of 𝑤𝑛 is defined as\n",
    "            # 𝑤𝑛|𝛽,𝜃 ~ Categorical(𝜎(𝛽𝜃))\n",
    "            count_param = self.decoder(theta)\n",
    "            # Currently, PyTorch Multinomial requires `total_count` to be homogeneous.\n",
    "            # Because the numbers of words across documents can vary,\n",
    "            # we will use the maximum count accross documents here.\n",
    "            # This does not affect the result because Multinomial.log_prob does\n",
    "            # not require `total_count` to evaluate the log probability.\n",
    "            total_count = int(docs.sum(-1).max())\n",
    "            pyro.sample(\n",
    "                'obs',\n",
    "                dist.Multinomial(total_count, count_param),\n",
    "                obs=docs\n",
    "            )\n",
    "            \n",
    "    def guide(self, docs):\n",
    "        pyro.module('encoder', self.encoder)\n",
    "        with pyro.plate('documents', docs.shape[0]):\n",
    "            logtheta_loc, logtheta_scale = self.encoder(docs)\n",
    "            logtheta = pyro.sample(\n",
    "                'logtheta', dist.Normal(logtheta_loc, logtheta_scale).to_event(1))\n",
    "            \n",
    "    def beta(self):\n",
    "        return self.decoder.beta.weight.cpu().detach().T\n",
    "    \n",
    "    def get_posterior_topic(self, docs):\n",
    "        logtheta_loc, logtheta_scale = self.encoder(docs)\n",
    "        theta_loc = F.softmax(logtheta_loc, -1)\n",
    "        return(theta_loc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4503073b-6825-42da-b796-1d3c2f0de677",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([18846, 12722])\n"
     ]
    }
   ],
   "source": [
    "seed = 0\n",
    "torch.manual_seed(seed)\n",
    "pyro.set_rng_seed(seed)\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "num_topics = 20\n",
    "docs = docs.float().to(device)\n",
    "batch_size = 32\n",
    "learning_rate = 1e-3\n",
    "num_epochs = 5 #50\n",
    "print(docs.shape) # torch.Size([18846, 12722]): (# documents, # words in the dictionary)--> values: counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "52838614-9e71-41a6-ba22-d01a238a77b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████| 5/5 [02:14<00:00, 26.96s/it, epoch_loss=4.08e+05]\n"
     ]
    }
   ],
   "source": [
    "pyro.clear_param_store()\n",
    "prodLDA = ProdLDA(\n",
    "    vocab_size = docs.shape[1],\n",
    "    num_topics = num_topics,\n",
    "    hidden = 100,\n",
    "    dropout=0.2\n",
    ")\n",
    "prodLDA.to(device)\n",
    "\n",
    "optimizer = pyro.optim.Adam({\"lr\": learning_rate})\n",
    "svi = SVI(prodLDA.model, prodLDA.guide, optimizer, loss=TraceMeanField_ELBO())\n",
    "num_batches = int(math.ceil(docs.shape[0] / batch_size))\n",
    "\n",
    "bar = trange(num_epochs)\n",
    "for epoch in bar:\n",
    "    running_loss = 0.0\n",
    "    for i in range(num_batches):\n",
    "        batch_docs = docs[i * batch_size:(i+1) * batch_size, :]\n",
    "        loss = svi.step(batch_docs)\n",
    "        running_loss += loss / batch_docs.size(0)\n",
    "        \n",
    "    bar.set_postfix(epoch_loss='{:.2e}'.format(running_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d2952b16-3d2a-4033-b1eb-926813b0bec2",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'wordcloud'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/tw/tl1cflf90x3dxc6h1rn8mbdm0000gn/T/ipykernel_78368/694860352.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mwordcloud\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mWordCloud\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0mbeta\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprodLDA\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbeta\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'wordcloud'"
     ]
    }
   ],
   "source": [
    "def plot_word_cloud(b, ax, v, n):\n",
    "    sorted_, indices = torch.sort(b, descending=True)\n",
    "    df = pd.DataFrame(indices[:100].numpy(), columns=['index'])\n",
    "    words = pd.merge(df, vocab[['index', 'word']],\n",
    "                     how='left', on='index')['word'].values.tolist()\n",
    "    sizes = (sorted_[:100] * 10000).int().numpy().tolist()\n",
    "    freqs = {words[i]: sizes[i] for i in range(len(words))}\n",
    "    wc = WordCloud(background_color=\"white\", width=800, height=500)\n",
    "    wc = wc.generate_from_frequencies(freqs)\n",
    "    ax.set_title('Topic %d' % (n+1))\n",
    "    ax.imshow(wc, interpolation='bilinear')\n",
    "    ax.axis('off')\n",
    "    \n",
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud # this does not work right now on my setup\n",
    "\n",
    "beta = prodLDA.beta()\n",
    "fig, axs = plt.subplots(7, 3, figsize=(14,24))\n",
    "for n in range(beta.shape[0]):\n",
    "    i, j = divmod(n, 3)\n",
    "    plot_word_cloud(beta[n], axs[i, j], vocab, n)\n",
    "axs[-1,-1].axis('off');\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "cfbb6742-8d16-4f9c-a37f-0bb71cfe3e03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([18846, 20])\n",
      "tensor(0.9999, grad_fn=<MaxBackward1>)\n",
      "tensor([1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
      "       grad_fn=<SumBackward1>)\n"
     ]
    }
   ],
   "source": [
    "topic_posterior = prodLDA.get_posterior_topic(docs)\n",
    "print(topic_posterior.shape)\n",
    "print(topic_posterior.max())\n",
    "print(topic_posterior.sum(axis = 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5b75f99-3c3d-45e5-aa7e-12669fed881a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
