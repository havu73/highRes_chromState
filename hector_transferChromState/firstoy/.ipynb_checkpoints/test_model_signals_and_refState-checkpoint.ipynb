{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "43c85389",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyro\n",
    "import pyro.distributions as dist\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import pyro\n",
    "import pyro.distributions as dist\n",
    "import torch.nn.functional as F\n",
    "from pyro.infer import SVI, TraceMeanField_ELBO\n",
    "from tqdm import trange\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95fdf15e",
   "metadata": {},
   "source": [
    "## Class to generate data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1e68399b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ToyGenerator:\n",
    "    def __init__(self,  \n",
    "                 num_bins=5, \n",
    "                 num_references=3, \n",
    "                 num_signals=3,\n",
    "                 num_states=3,\n",
    "                 high_w=100):\n",
    "        self.num_bins = num_bins\n",
    "        self.num_references = num_references\n",
    "        self.num_signals = num_signals\n",
    "        self.num_states = num_states\n",
    "        self.high_w = high_w\n",
    "        self.sample = None\n",
    "        self.params = self.set_params()\n",
    "    \n",
    "        \n",
    "    # parameter of state->signal distributions\n",
    "    # shape is (num_states, num_signals)\n",
    "    def generate_param_p(self):\n",
    "        p = torch.zeros((self.num_states, self.num_signals))\n",
    "        for i in range(self.num_states):\n",
    "            w = -self.high_w * torch.ones(self.num_signals)\n",
    "            w[i % self.num_signals] = self.high_w\n",
    "            p[i,:] = w\n",
    "        return p\n",
    "    \n",
    "    # generate a state assignment tensor\n",
    "    # shape is (num_bins, num_references)\n",
    "    # helper function for set_params\n",
    "    def generate_ref_states(self):\n",
    "        ref_states = torch.zeros(\n",
    "            (self.num_bins, \n",
    "             self.num_references))\n",
    "        \n",
    "        for i in range(self.num_references):\n",
    "            ref_states[:,i] = i % self.num_states\n",
    "        return ref_states.long()\n",
    "    \n",
    "    # set parameters of the data generator\n",
    "    def set_params(self):\n",
    "        # parameters of the dirichlet over references\n",
    "        # same one for every region\n",
    "        # very high probability that generated sample looks like\n",
    "        # reference 0\n",
    "        # shape is (num_references,)\n",
    "        alpha = torch.ones(self.num_references)\n",
    "        # alpha[0] = self.high_w\n",
    "        \n",
    "        # parameters of bernoulli distribution for each signal\n",
    "        # for each state\n",
    "        # shape is (num_states, num_signals)\n",
    "        p = self.generate_param_p()\n",
    "        \n",
    "        # an indicator matrix along genome of the state for \n",
    "        # each refenrece\n",
    "        # shape is (num_regions, num_bins_per_region, num_states, num_references)\n",
    "        ref_states_indicator = F.one_hot(self.generate_ref_states(), self.num_states)\n",
    "        params = {\n",
    "            'alpha': alpha,\n",
    "            'p': p,\n",
    "            'ref_states_indicator': ref_states_indicator\n",
    "        }\n",
    "        self.params = params\n",
    "        return params\n",
    "        \n",
    "    # collapse a prob vector over references to a prob vector over states\n",
    "    # takes the cross product of prob vector theta and reference state indicator matrix r\n",
    "    # shapes:\n",
    "    #  theta: (None, num_references)\n",
    "    #  r: (None, num_references, num_states)\n",
    "    #  out: (None, num_states)\n",
    "    def collapse_theta(self, theta, r=None):\n",
    "        if r is None:\n",
    "            assert self.params is not None\n",
    "            r = self.params['ref_states_indicator']\n",
    "            \n",
    "        r = r.float()\n",
    "        collapsed_theta = torch.zeros(theta.shape[0], r.shape[2])\n",
    "        for i in range(theta.shape[0]):\n",
    "            collapsed_theta[i,:] = torch.matmul(r[i,:,:].T, theta[i,:])\n",
    "        return collapsed_theta\n",
    "    \n",
    "    def generate_sample(self):\n",
    "        if self.params is None:\n",
    "            self.set_params()\n",
    "            \n",
    "        r = self.params['ref_states_indicator']\n",
    "                \n",
    "        # generate reference distribution for each region\n",
    "        with pyro.plate('bins', self.num_bins):\n",
    "            # theta is shape (num_regions, num_references)\n",
    "            theta = pyro.sample('theta', dist.Dirichlet(self.params['alpha']))\n",
    "            # collapse the reference distribution for each bin to a \n",
    "            # state distribution \n",
    "            collapsed_theta = self.collapse_theta(theta, r)\n",
    "\n",
    "            signal_params = torch.sigmoid(torch.matmul(collapsed_theta, self.params['p']))\n",
    "            m = pyro.sample('m', dist.Bernoulli(signal_params).to_event(1))\n",
    "\n",
    "        result = {\n",
    "            'theta': theta,\n",
    "            'm': m,\n",
    "        }\n",
    "        self.sample = result\n",
    "        return self.sample\n",
    "    \n",
    "    def get_sampled_collapsed_theta(self):\n",
    "        if self.sample is None:\n",
    "            self.generate_sample()\n",
    "        theta = self.sample['theta']\n",
    "        return self.collapse_theta(theta)\n",
    "    \n",
    "    def get_sampled_signals(self):\n",
    "        if self.sample is None:\n",
    "            self.generate_sample()\n",
    "        return self.sample['m']\n",
    "    \n",
    "    def get_sampled_theta(self):\n",
    "        if self.sample is None:\n",
    "            self.generate_sample()\n",
    "        return self.sample['theta']\n",
    "    \n",
    "    def get_signal_parms(self):\n",
    "        collapsed_theta = self.get_sampled_collapsed_theta()\n",
    "        return torch.sigmoid(torch.matmul(collapsed_theta, self.params['p']))\n",
    "    \n",
    "    def get_ref_state_indicators(self):\n",
    "        if self.params is None:\n",
    "            self.set_params()\n",
    "        return self.params['ref_states_indicator']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ee180c7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "M: # regions\n",
    "N: # bins per region\n",
    "L: # signals (marks)\n",
    "alpha: params of dirichlet prior over reference epigenomics\n",
    "beta: ref --> sample state categorical distribution\n",
    "p: state --> signal bernoulli distribution \n",
    "r: reference state at each bin. one-hot encoding, matrix size : #bins * #ref * #states\n",
    "theta: the mixture probabilities of reference ethetagenome\n",
    "'''\n",
    "\n",
    "class CircularStateGenerator:\n",
    "    # Within the number of references, there is a group of references that will be similar to the \n",
    "    # sample of interests in terms of state assignments\n",
    "    def __init__(self,  \n",
    "                 num_bins=5, \n",
    "                 num_references=10, \n",
    "                 num_groups=3,\n",
    "                 state_vary_rate=0.01, \n",
    "                 # fraction of the genome where the state assignments among references of the same group are diff\n",
    "                 num_signals=3,\n",
    "                 num_states=5,\n",
    "                 high_w=100):\n",
    "        self.num_bins = num_bins\n",
    "        self.num_references = num_references\n",
    "        self.num_groups = num_groups\n",
    "        self.state_vary_rate = state_vary_rate\n",
    "        self.num_signals = num_signals\n",
    "        self.num_states = num_states\n",
    "        self.high_w = high_w\n",
    "        self.sample = None\n",
    "        self.params = self.set_params()\n",
    "    \n",
    "        \n",
    "    # parameter of state->signal distributions\n",
    "    # shape is (num_states, num_signals)\n",
    "    def generate_param_p(self):\n",
    "        p = torch.zeros((self.num_states, self.num_signals))\n",
    "        for i in range(self.num_states):\n",
    "            w = -self.high_w * torch.ones(self.num_signals)\n",
    "            w[i % self.num_signals] = self.high_w\n",
    "            p[i,:] = w\n",
    "        return p\n",
    "    \n",
    "    # generate a state assignment tensor\n",
    "    # shape is (num_regions, num_bins_per_region, num_references)\n",
    "    def generate_ref_states(self):\n",
    "        # this is code for the case where we want varied state patterns from each reference\n",
    "        # and that there are actually groups of references that are similar to each other\n",
    "        num_ref_per_groups = np.ceil(self.num_references/self.num_groups).astype(int)\n",
    "        sample_r = torch.zeros(self.num_states, self.num_groups)\n",
    "        for i in range(self.num_groups):\n",
    "            sample_r[:,i] = torch.arange(self.num_states).roll(i)\n",
    "            # each group has a circular permutation of states that are characteristics to that group\n",
    "        sample_r = sample_r.repeat(np.ceil(self.num_bins / self.num_states).astype(int), 1)\n",
    "        # now r is just a repeated sequence of sample_r\n",
    "        r = torch.zeros(sample_r.shape[0], self.num_references)\n",
    "        for i in range(self.num_references):\n",
    "            r[:,i] = sample_r[:, i % self.num_groups]\n",
    "        # now we will start to introduce some random changes to the state assignments among references from\n",
    "        # the same groups\n",
    "        num_change = int(self.state_vary_rate * self.num_bins)\n",
    "        for i in range(self.num_states, self.num_references): \n",
    "            # for the first num_states columns, keep all the state assignments\n",
    "            # if num_references < num_states, this loop will not be called\n",
    "            org_r = r[:,i]\n",
    "            indices_to_change = np.random.choice(self.num_bins, num_change)\n",
    "            indices_to_change = torch.tensor(indices_to_change).type(torch.LongTensor)\n",
    "            states_to_change = torch.tensor(np.random.choice(self.num_states, num_change)).float()\n",
    "            r[indices_to_change,i] = states_to_change\n",
    "        r = r[:self.num_bins,:self.num_references]\n",
    "        return r.long() # num_bins, num_references --> values: state-0-based \n",
    "    \n",
    "    # set parameters of the data generator\n",
    "    def set_params(self):\n",
    "        # parameters of the dirichlet over references\n",
    "        # same one for every region\n",
    "        # very high probability that generated sample looks like\n",
    "        # reference 0\n",
    "        # shape is (num_references,)\n",
    "        alpha = torch.ones(self.num_references)\n",
    "        num_ref_per_groups = np.ceil(self.num_references/self.num_groups).astype(int)\n",
    "        for i in range(self.num_references):\n",
    "            if i % self.num_groups == 0:\n",
    "                alpha[i] = self.high_w # all refs in group 1 will be more similar to sample of interest\n",
    "        \n",
    "        # parameters of bernoulli distribution for each signal\n",
    "        # for each state\n",
    "        # shape is (num_states, num_signals)\n",
    "        p = self.generate_param_p()\n",
    "        \n",
    "        # an indicator matrix along genome of the state for \n",
    "        # each refenrece\n",
    "        # shape is (num_regions, num_bins_per_region, num_states, num_references)\n",
    "        ref_states_indicator = F.one_hot(self.generate_ref_states(), self.num_states)\n",
    "        params = {\n",
    "            'alpha': alpha,\n",
    "            'p': p,\n",
    "            'ref_states_indicator': ref_states_indicator\n",
    "        }\n",
    "        self.params = params\n",
    "        return params\n",
    "        \n",
    "    # collapse a prob vector over references to a prob vector over states\n",
    "    # takes the cross product of prob vector theta and reference state indicator matrix r\n",
    "    # shapes:\n",
    "    #  theta: (None, num_references)\n",
    "    #  r: (None, num_references, num_states)\n",
    "    #  out: (None, num_states)\n",
    "    def collapse_theta(self, theta, r=None):\n",
    "        if r is None:\n",
    "            assert self.params is not None\n",
    "            r = self.params['ref_states_indicator']\n",
    "            \n",
    "        r = r.float()\n",
    "        collapsed_theta = torch.zeros(theta.shape[0], r.shape[2])\n",
    "        for i in range(theta.shape[0]):\n",
    "            collapsed_theta[i,:] = torch.matmul(r[i,:,:].T, theta[i,:])\n",
    "        return collapsed_theta\n",
    "    \n",
    "    def generate_sample(self):\n",
    "        if self.params is None:\n",
    "            self.set_params()\n",
    "            \n",
    "        r = self.params['ref_states_indicator']\n",
    "                \n",
    "        # generate reference distribution for each region\n",
    "        with pyro.plate('bins', self.num_bins):\n",
    "            # theta is shape (num_regions, num_references)\n",
    "            theta = pyro.sample('theta', dist.Dirichlet(self.params['alpha']))\n",
    "            # collapse the reference distribution for each bin to a \n",
    "            # state distribution \n",
    "            collapsed_theta = self.collapse_theta(theta, r)\n",
    "\n",
    "            signal_params = torch.sigmoid(torch.matmul(collapsed_theta, self.params['p']))\n",
    "            m = pyro.sample('m', dist.Bernoulli(signal_params).to_event(1))\n",
    "\n",
    "        result = {\n",
    "            'theta': theta,\n",
    "            'm': m\n",
    "        }\n",
    "        self.sample = result\n",
    "        return self.sample\n",
    "    \n",
    "\n",
    "    def get_sampled_collapsed_theta(self):\n",
    "        if self.sample is None:\n",
    "            self.generate_sample()\n",
    "        theta = self.sample['theta']\n",
    "        return self.collapse_theta(theta)\n",
    "    \n",
    "    def get_sampled_signals(self):\n",
    "        if self.sample is None:\n",
    "            self.generate_sample()\n",
    "        return self.sample['m']\n",
    "    \n",
    "    def get_sampled_theta(self):\n",
    "        if self.sample is None:\n",
    "            self.generate_sample()\n",
    "        return self.sample['theta']\n",
    "    \n",
    "    def get_signal_parms(self):\n",
    "        collapsed_theta = self.get_sampled_collapsed_theta()\n",
    "        return torch.sigmoid(torch.matmul(collapsed_theta, self.params['p']))\n",
    "    \n",
    "    def get_ref_state_indicators(self):\n",
    "        if self.params is None:\n",
    "            self.set_params()\n",
    "        return self.params['ref_states_indicator']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e5db298a",
   "metadata": {},
   "outputs": [],
   "source": [
    "serious_parms = {\n",
    "    'num_bins': 10000,\n",
    "    'num_references': 10,\n",
    "    'num_groups': 3,\n",
    "    'state_vary_rate': 0.003,\n",
    "    'num_signals': 3,\n",
    "    'num_states': 3,\n",
    "    'high_w': 100\n",
    "}\n",
    "\n",
    "seed = 0\n",
    "torch.manual_seed(seed)\n",
    "pyro.set_rng_seed(seed)\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "generator = CircularStateGenerator(**serious_parms)\n",
    "\n",
    "m = generator.get_sampled_signals()\n",
    "r = generator.get_ref_state_indicators()\n",
    "collapsed_theta = generator.get_sampled_collapsed_theta()\n",
    "theta = generator.get_sampled_theta()\n",
    "signal_params = generator.get_signal_parms()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "58a4aa33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'num_bins': 10000, 'num_references': 10, 'num_groups': 3, 'state_vary_rate': 0.003, 'num_signals': 3, 'num_states': 3, 'high_w': 100}\n",
      "m: obs. signals at each position\n",
      "torch.Size([10000, 3])\n",
      "tensor([[1., 0., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 0., 1.],\n",
      "        [1., 0., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 0., 1.],\n",
      "        [1., 0., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 0., 1.],\n",
      "        [1., 0., 0.]])\n",
      "r: reference epigenome state indicator at each position\n",
      "torch.Size([10000, 10, 3])\n",
      "tensor([[0, 1, 0],\n",
      "        [0, 0, 1],\n",
      "        [1, 0, 0],\n",
      "        [0, 1, 0],\n",
      "        [0, 0, 1],\n",
      "        [1, 0, 0],\n",
      "        [0, 1, 0],\n",
      "        [0, 0, 1],\n",
      "        [1, 0, 0],\n",
      "        [0, 1, 0]])\n",
      "collapsed_theta: state assignment at each position\n",
      "torch.Size([10000, 3])\n",
      "tensor([[0.9912, 0.0020, 0.0068],\n",
      "        [0.0109, 0.9862, 0.0029],\n",
      "        [0.0024, 0.0276, 0.9700],\n",
      "        ...,\n",
      "        [0.0027, 0.9882, 0.0091],\n",
      "        [0.0047, 0.0035, 0.9917],\n",
      "        [0.9931, 0.0049, 0.0019]])\n",
      "theta: the reference mixture at each position\n",
      "torch.Size([10000, 10])\n",
      "tensor([[2.8926e-01, 1.1351e-03, 2.8802e-04,  ..., 5.0673e-03, 7.5635e-04,\n",
      "         2.5826e-01],\n",
      "        [2.4719e-01, 6.4616e-03, 1.9981e-03,  ..., 2.9050e-03, 3.1032e-04,\n",
      "         2.2649e-01],\n",
      "        [2.3822e-01, 2.2335e-03, 2.7438e-04,  ..., 1.5360e-02, 6.2279e-04,\n",
      "         2.3243e-01],\n",
      "        ...,\n",
      "        [2.3123e-01, 1.0679e-03, 3.4313e-03,  ..., 4.5041e-04, 3.5226e-03,\n",
      "         2.2472e-01],\n",
      "        [2.4472e-01, 2.5040e-06, 1.5063e-03,  ..., 1.4921e-03, 3.1532e-03,\n",
      "         2.5970e-01],\n",
      "        [2.7309e-01, 7.5642e-04, 3.5097e-03,  ..., 2.9428e-04, 1.0766e-03,\n",
      "         2.5016e-01]])\n",
      "signal_params: bernoulli dist. params generating signal at each position\n",
      "torch.Size([10000, 3])\n",
      "p\n",
      "tensor([[ 100., -100., -100.],\n",
      "        [-100.,  100., -100.],\n",
      "        [-100., -100.,  100.]])\n"
     ]
    }
   ],
   "source": [
    "print(serious_parms)\n",
    "print('m: obs. signals at each position')\n",
    "print(m.shape)\n",
    "print(m[:10,:])\n",
    "print('r: reference epigenome state indicator at each position')\n",
    "print(r.shape)\n",
    "print(r[:10,2,:])\n",
    "print('collapsed_theta: state assignment at each position')\n",
    "print(collapsed_theta.shape)\n",
    "print(collapsed_theta)\n",
    "print('theta: the reference mixture at each position')\n",
    "print(theta.shape)\n",
    "print(theta)\n",
    "print('signal_params: bernoulli dist. params generating signal at each position')\n",
    "print(signal_params.shape)\n",
    "print('p')\n",
    "p = generator.params['p']\n",
    "print (p)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ca45d04",
   "metadata": {},
   "source": [
    "## Model with data of signals and ref states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e45ebcc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "\tdef __init__(self, num_signals, num_states, num_references, hidden, dropout):\n",
    "\t\tsuper().__init__()\n",
    "\t\tself.drop = nn.Dropout(dropout)\n",
    "\t\tinput_dim = num_signals + num_states * num_references\n",
    "\t\tself.fc1 = nn.Linear(input_dim, hidden)\n",
    "\t\tself.fc2 = nn.Linear(hidden, hidden)\n",
    "\t\tself.fcmu = nn.Linear(hidden, num_states)\n",
    "\t\tself.fclv = nn.Linear(hidden, num_states)\n",
    "\n",
    "\tdef forward(self, m, r):\n",
    "\t\tinputs = torch.cat((m, r.reshape(r.shape[0], -1)), 1)\n",
    "\t\th = F.softplus(self.fc1(inputs))\n",
    "\t\th = F.softplus(self.fc2(h))\n",
    "\t\th = self.drop(h)\n",
    "\t\tlogpi_loc = (self.fcmu(h))\n",
    "\t\tlogpi_logvar = self.fclv(h)\n",
    "\t\tlogpi_scale = (0.5 * logpi_logvar).exp()\n",
    "\t\treturn logpi_loc, logpi_scale\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "\tdef __init__(self, num_states, num_signals, num_references, hidden, dropout):\n",
    "\t\tsuper().__init__()\n",
    "\t\tself.num_states = num_states\n",
    "\t\tself.num_signals = num_signals\n",
    "\t\tself.num_references = num_references\n",
    "\t\tself.drop = nn.Dropout(dropout)\n",
    "\t\tself.fcih = nn.Linear(num_states, hidden) # input (state probabilities) --> hidden\n",
    "\t\tself.fchh = nn.Linear(hidden, hidden) # hiddent --> hidden\n",
    "\t\tself.fchs = nn.Linear(hidden, self.num_signals) # hidden --> signals\n",
    "\t\tself.fchr = nn.Linear(hidden, self.num_references * self.num_states) # hidden --> states in references\n",
    "\n",
    "\n",
    "\n",
    "\tdef forward(self, inputs):\n",
    "\t\t# takes in the values of collapsed pi: probabilities of state \n",
    "\t\t# assignments at each positions, and then apply a linear trans\n",
    "\t\t# to get the probabilities of observing signals at each position\n",
    "\t\t# --> vector size #signals\n",
    "\t\t# used as parameters for bernoulli dist. to get obs. signals\n",
    "\t\t# create multiple layers\n",
    "\t\t# inputs: bins, state probabilities\n",
    "\t\t# h: bins, hidden \n",
    "\t\t# signal_param: bins, signals\n",
    "\t\t# ref_param: bins, num_references, num_states\n",
    "\t\th = F.softplus(self.fcih(inputs)) # --> hidden element vector\n",
    "\t\th = F.softplus(self.fchh(h)) # --> hidden element vector\n",
    "\t\th = self.drop(h)\n",
    "\t\tsignal_param = torch.sigmoid(self.fchs(h)) # hidden --> marks\n",
    "\t\tref_param = torch.sigmoid(self.fchr(h)).reshape((h.shape[0], self.num_references, self.num_states)) \n",
    "\t\t# hidden --> num_ref*num_states\n",
    "\t\tref_param = F.normalize(ref_param, p = 1.0, dim = 2) # row normalize, sum over states per ref is 1\n",
    "\t\treturn signal_param, ref_param\n",
    "\n",
    "class Model_signals_refStates(nn.Module):\n",
    "\tdef __init__(self, num_signals, num_references, num_states, hidden, dropout):\n",
    "\t\tsuper().__init__()\n",
    "\t\tself.num_signals = num_signals\n",
    "\t\tself.num_references = num_references\n",
    "\t\tself.num_states = num_states\n",
    "\t\tself.hidden = hidden\n",
    "\t\tself.dropout = dropout\n",
    "\t\tself.encoder = Encoder(num_signals, num_states, num_references, hidden, dropout)\n",
    "\t\tself.decoder = Decoder(num_states, num_signals, num_references, hidden, dropout)\n",
    "\n",
    "\t# shapes: \n",
    "\t#  m: (bins x signals) signal matrix\n",
    "\t#  r: (bins x reference x state) indicator matrix\n",
    "\tdef model(self, m, r):\n",
    "\t\t# flatten out the r indicator matrix\n",
    "\t\tpyro.module(\"decoder\", self.decoder)\n",
    "\t\twith pyro.plate('bins', m.shape[0]):\n",
    "\t\t\tlogCpi_loc = m.new_zeros((m.shape[0], self.num_states))\n",
    "\t\t\tlogCpi_scale = m.new_ones((m.shape[0], self.num_states))\n",
    "\t\t\tlogCpi = pyro.sample('log_collapsedPi', dist.Normal(logCpi_loc, logCpi_scale).to_event(1))\n",
    "\t\t\tCpi = F.softmax(logCpi, -1) \n",
    "\t\t\t# the softmax function should be used here because Cpi is lognormal\n",
    "\t\t\tsignal_param, ref_param = self.decoder(Cpi) # vector of probabilities. \n",
    "\t\t\t# signal_param: bins, signals\n",
    "\t\t\t# ref_param: bins, references, states\n",
    "\t\t\t# first num_signals elements: bernoulli params\n",
    "\t\t\t# each of the following num_states elements: multinomial params\n",
    "\t\t\t# for the state segmentation in a reference    \n",
    "\t\t\tpyro.sample('m', dist.Bernoulli(signal_param).to_event(1), obs=m)\n",
    "\t\t\t# plate across references\n",
    "\t\t\twith pyro.plate('refs', self.num_references):\n",
    "\t\t\t\tpyro.sample('r', dist.Multinomial(1, ref_param).to_event(1), obs = r)\n",
    "\n",
    "\tdef guide(self, m, r):\n",
    "\t\tpyro.module(\"encoder\", self.encoder)\n",
    "\t\twith pyro.plate('bins', m.shape[0]):\n",
    "\t\t\tlogpi_loc, logpi_scale = self.encoder(m, r)\n",
    "\t\t\tlogpi = pyro.sample('log_collapsedPi', dist.Normal(logpi_loc, logpi_scale).to_event(1))\n",
    "\n",
    "\tdef predict_state_assignment(self, m, r):\n",
    "\t\tlogpi_loc, logpi_scale = self.encoder(m, r)\n",
    "\t\tCpi = F.softmax(logpi_loc, -1)\n",
    "\t\treturn(Cpi)\n",
    "\n",
    "\n",
    "\tdef generate_reconstructed_data(self, m, r):\n",
    "\t\tlogpi_loc, logpi_scale = self.encoder(m, r)\n",
    "\t\tCpi = F.softmax(logpi_loc, -1)\n",
    "\t\tsignal_param, ref_param = self.decoder(Cpi) # vector of probabilities. \n",
    "\t\tre_m = pyro.sample('re_m', dist.Bernoulli(signal_param).to_event(1))\n",
    "\t\tre_r = pyro.sample('re_r', dist.Multinomial(1, ref_param).to_event(1))\n",
    "\t\treturn(re_m, re_r)\n",
    "\n",
    "\n",
    "\tdef get_percentage_correct_reconstruct(self, m, r):\n",
    "\t\t# m and r can be different from the m and r used in training\n",
    "\t\tre_m, re_r = self.generate_reconstructed_data(m,r)\n",
    "\t\ttotal_m_entries = re_m.shape[0] * re_m.shape[1]\n",
    "\t\tsignals_CR = (re_m==m).sum() # correct reconstruct entries of signals\n",
    "\t\ttotal_r_entries = re_r.shape[0] * self.num_references\n",
    "\t\t# for each reference at each position, if the state assignment is different between re_r and r, there are 2 out of num_states entries that are different between re_r and r\n",
    "\t\twrong_r = ((re_r.shape[0] * re_r.shape[1] * re_r.shape[2]) - (re_r==r).sum()) / 2 # wrong reconstruct entries of reference states\n",
    "\t\tr_CR = total_r_entries - wrong_r\n",
    "\t\tratio_m_CR = (signals_CR / total_m_entries).item()\n",
    "\t\tratio_r_CR = (r_CR / total_r_entries).item()\n",
    "\t\treturn ratio_m_CR, ratio_r_CR\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f1796953",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████| 1000/1000 [07:56<00:00,  2.10it/s, epoch_loss=1.79e+02]\n"
     ]
    }
   ],
   "source": [
    "batch_size = 200\n",
    "learning_rate = 1e-3\n",
    "num_epochs = 1000\n",
    "pyro.clear_param_store()\n",
    "state_model = Model_signals_refStates(\n",
    "    num_signals = generator.num_signals,\n",
    "    num_references = generator.num_references,\n",
    "    num_states = generator.num_states,\n",
    "    hidden = 32,\n",
    "    dropout = 0.2)\n",
    "state_model.to(device)\n",
    "optimizer = pyro.optim.Adam({\"lr\": learning_rate})\n",
    "svi = SVI(state_model.model, state_model.guide, optimizer, loss=TraceMeanField_ELBO())\n",
    "num_batches = int(math.ceil(m.shape[0] / batch_size))\n",
    "\n",
    "bar = trange(num_epochs)\n",
    "for epoch in bar:\n",
    "    running_loss = 0.0\n",
    "    for i in range(num_batches):\n",
    "        batch_m = m[i * batch_size:(i+1) * batch_size, :]\n",
    "        batch_r = r[i * batch_size:(i+1) * batch_size, :, :]\n",
    "        loss = svi.step(batch_m, batch_r)\n",
    "        running_loss += loss / batch_m.size(0)\n",
    "        \n",
    "    bar.set_postfix(epoch_loss='{:.2e}'.format(running_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6bc602c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(208.)\n"
     ]
    }
   ],
   "source": [
    "# ratio_m_CR, ratio_r_CR = state_model.get_percentage_correct_reconstruct(m,r)\n",
    "re_m, re_r = state_model.generate_reconstructed_data(m,r)\n",
    "wrong_r = ((re_r.shape[0] * re_r.shape[1] * re_r.shape[2]) - (re_r==r).sum()) / 2\n",
    "print(wrong_r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "23448557",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10000, 3])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "Cpi = state_model.predict_state_assignment(m, r)\n",
    "print(Cpi.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e8c7b040",
   "metadata": {},
   "outputs": [],
   "source": [
    "logpi_loc, logpi_scale = state_model.encoder(m, r)\n",
    "Cpi = F.softmax(logpi_loc, -1)\n",
    "signal_param, ref_param = state_model.decoder(Cpi) # vector of probabilities. \n",
    "re_m = pyro.sample('re_m', dist.Bernoulli(signal_param).to_event(1))\n",
    "re_r = pyro.sample('re_r', dist.Multinomial(1, ref_param).to_event(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a63ec7ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             0         1         2\n",
      "0     0.743172  0.135145  0.121683\n",
      "1     0.121452  0.112941  0.765607\n",
      "2     0.132993  0.740218  0.126790\n",
      "3     0.723228  0.128154  0.148618\n",
      "4     0.117799  0.110641  0.771560\n",
      "...        ...       ...       ...\n",
      "9995  0.096648  0.807010  0.096343\n",
      "9996  0.773625  0.115915  0.110460\n",
      "9997  0.077738  0.071258  0.851004\n",
      "9998  0.072391  0.831101  0.096508\n",
      "9999  0.816641  0.091635  0.091724\n",
      "\n",
      "[10000 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "print(pd.DataFrame(Cpi.detach().numpy()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5910f88a",
   "metadata": {},
   "source": [
    "## Model with signals and ref states, fixed beta values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a8139087",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "\tdef __init__(self, num_signals, num_states, num_references, hidden, dropout):\n",
    "\t\tsuper().__init__()\n",
    "\t\tself.drop = nn.Dropout(dropout)\n",
    "\t\tinput_dim = num_signals + num_states * num_references\n",
    "\t\tself.fc1 = nn.Linear(input_dim, hidden)\n",
    "\t\tself.fc2 = nn.Linear(hidden, hidden)\n",
    "\t\tself.fcmu = nn.Linear(hidden, num_states)\n",
    "\t\tself.fclv = nn.Linear(hidden, num_states)\n",
    "\n",
    "\tdef forward(self, m, r):\n",
    "\t\tinputs = torch.cat((m, r.reshape(r.shape[0], -1)), 1)\n",
    "\t\th = F.softplus(self.fc1(inputs))\n",
    "\t\th = F.softplus(self.fc2(h))\n",
    "\t\th = self.drop(h)\n",
    "\t\tlogpi_loc = (self.fcmu(h))\n",
    "\t\tlogpi_logvar = self.fclv(h)\n",
    "\t\tlogpi_scale = (0.5 * logpi_logvar).exp()\n",
    "\t\treturn logpi_loc, logpi_scale\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "\tdef __init__(self, num_states, num_signals, num_references, hidden, dropout, fixed_signalP):\n",
    "\t\tsuper().__init__()\n",
    "\t\tself.num_states = num_states\n",
    "\t\tself.num_signals = num_signals\n",
    "\t\tself.num_references = num_references\n",
    "\t\tself.fixed_signalP = fixed_signalP\n",
    "\t\tself.drop = nn.Dropout(dropout)\n",
    "\t\tself.fcih = nn.Linear(num_states, hidden) # input (state probabilities) --> hidden\n",
    "\t\tself.fchh = nn.Linear(hidden, hidden) # hiddent --> hidden\n",
    "\t\tself.fchs = nn.Linear(hidden, self.num_signals) # hidden --> signals\n",
    "\t\tself.fchr = nn.Linear(hidden, self.num_references * self.num_states) # hidden --> states in references\n",
    "\n",
    "\n",
    "\n",
    "\tdef forward(self, inputs):\n",
    "\t\t# takes in the values of collapsed pi: probabilities of state \n",
    "\t\t# assignments at each positions, and then apply a linear trans\n",
    "\t\t# to get the probabilities of observing signals at each position\n",
    "\t\t# --> vector size #signals\n",
    "\t\t# used as parameters for bernoulli dist. to get obs. signals\n",
    "\t\t# create multiple layers\n",
    "\t\t# inputs: bins, state probabilities\n",
    "\t\t# h: bins, hidden \n",
    "\t\t# signal_param: bins, signals\n",
    "\t\t# ref_param: bins, num_references, num_states\n",
    "\t\th = F.softplus(self.fcih(inputs)) # --> hidden element vector\n",
    "\t\th = F.softplus(self.fchh(h)) # --> hidden element vector\n",
    "\t\th = self.drop(h)\n",
    "\t\tsignal_param = torch.sigmoid(torch.matmul(inputs, self.fixed_signalP)) # hidden --> marks\n",
    "\t\tref_param = torch.sigmoid(self.fchr(h)).reshape((h.shape[0], self.num_references, self.num_states)) \n",
    "\t\t# hidden --> num_ref*num_states\n",
    "\t\tref_param = F.normalize(ref_param, p = 1.0, dim = 2) # row normalize, sum over states per ref is 1\n",
    "\t\treturn signal_param, ref_param\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "class Model_signals_refStates_fixedBeta(nn.Module):\n",
    "\tdef __init__(self, num_signals, num_references, num_states, hidden, dropout, fixed_signalP):\n",
    "\t\tsuper().__init__()\n",
    "\t\tself.num_signals = num_signals\n",
    "\t\tself.num_references = num_references\n",
    "\t\tself.num_states = num_states\n",
    "\t\tself.hidden = hidden\n",
    "\t\tself.dropout = dropout\n",
    "\t\tself.fixed_signalP = fixed_signalP\n",
    "\t\tself.encoder = Encoder(num_signals, num_states, num_references, hidden, dropout)\n",
    "\t\tself.decoder = Decoder(num_states, num_signals, num_references, hidden, dropout, fixed_signalP)\n",
    "\n",
    "\t# shapes: \n",
    "\t#  m: (bins x signals) signal matrix\n",
    "\t#  r: (bins x reference x state) indicator matrix\n",
    "\tdef model(self, m, r):\n",
    "\t\t# flatten out the r indicator matrix\n",
    "\t\tpyro.module(\"decoder\", self.decoder)\n",
    "\t\twith pyro.plate('bins', m.shape[0]):\n",
    "\t\t\tlogCpi_loc = m.new_zeros((m.shape[0], self.num_states))\n",
    "\t\t\tlogCpi_scale = m.new_ones((m.shape[0], self.num_states))\n",
    "\t\t\tlogCpi = pyro.sample('log_collapsedPi', dist.Normal(logCpi_loc, logCpi_scale).to_event(1))\n",
    "\t\t\tCpi = F.softmax(logCpi, -1) \n",
    "\t\t\t# the softmax function should be used here because Cpi is lognormal\n",
    "\t\t\tsignal_param, ref_param = self.decoder(Cpi) # vector of probabilities. \n",
    "\t\t\t# signal_param: bins, signals\n",
    "\t\t\t# ref_param: bins, references, states\n",
    "\t\t\t# first num_signals elements: bernoulli params\n",
    "\t\t\t# each of the following num_states elements: multinomial params\n",
    "\t\t\t# for the state segmentation in a reference    \n",
    "\t\t\tt1 = pyro.sample('m', dist.Bernoulli(signal_param).to_event(1), obs=m)\n",
    "\t\t\t# plate across references\n",
    "\t\t\twith pyro.plate('refs', self.num_references):\n",
    "\t\t\t\tt2 = pyro.sample('r', dist.Multinomial(1, ref_param).to_event(1), obs = r)\n",
    "\n",
    "\tdef guide(self, m, r):\n",
    "\t\tpyro.module(\"encoder\", self.encoder)\n",
    "\t\twith pyro.plate('bins', m.shape[0]):\n",
    "\t\t\tlogpi_loc, logpi_scale = self.encoder(m, r)\n",
    "\t\t\tlogpi = pyro.sample('log_collapsedPi', dist.Normal(logpi_loc, logpi_scale).to_event(1))\n",
    "\n",
    "\tdef predict_state_assignment(self, m, r):\n",
    "\t\tlogpi_loc, logpi_scale = self.encoder(m, r)\n",
    "\t\tCpi = F.softmax(logpi_loc, -1)\n",
    "\t\treturn(Cpi)\n",
    "\n",
    "\n",
    "\tdef generate_reconstructed_data(self, m, r):\n",
    "\t\tlogpi_loc, logpi_scale = self.encoder(m, r)\n",
    "\t\tCpi = F.softmax(logpi_loc, -1)\n",
    "\t\tsignal_param, ref_param = self.decoder(Cpi) # vector of probabilities. \n",
    "\t\tre_m = pyro.sample('re_m', dist.Bernoulli(signal_param).to_event(1))\n",
    "\t\tre_r = pyro.sample('re_r', dist.Multinomial(1, ref_param).to_event(1))\n",
    "\t\treturn(re_m, re_r)\n",
    "\n",
    "\n",
    "\tdef get_percentage_correct_reconstruct(self, m, r):\n",
    "\t\t# m and r can be different from the m and r used in training\n",
    "\t\tre_m, re_r = self.generate_reconstructed_data(m,r)\n",
    "\t\ttotal_m_entries = re_m.shape[0] * re_m.shape[1]\n",
    "\t\tsignals_CR = (re_m==m).sum() # correct reconstruct entries of signals\n",
    "\t\ttotal_r_entries = re_r.shape[0] * self.num_references\n",
    "\t\t# for each reference at each position, if the state assignment is different between re_r and r, there are 2 out of num_states entries that are different between re_r and r\n",
    "\t\twrong_r = ((re_r.shape[0] * re_r.shape[1] * re_r.shape[2]) - (re_r==r).sum()) / 2 # wrong reconstruct entries of reference states\n",
    "\t\tr_CR = total_r_entries - wrong_r\n",
    "\t\tratio_m_CR = (signals_CR / total_m_entries).item()\n",
    "\t\tratio_r_CR = (r_CR / total_r_entries).item()\n",
    "\t\treturn ratio_m_CR, ratio_r_CR\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a64097f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                  | 0/1000 [00:00<?, ?it/s]/opt/anaconda3/envs/havu_2/lib/python3.7/site-packages/pyro/util.py:361: UserWarning: Found plate statements in guide but not model: {'regions'}\n",
      "  guide_vars - model_vars\n",
      "  4%|▊                   | 42/1000 [00:16<06:21,  2.51it/s, epoch_loss=2.58e+02]"
     ]
    }
   ],
   "source": [
    "batch_size = 200\n",
    "learning_rate = 1e-3\n",
    "num_epochs = 1000\n",
    "pyro.clear_param_store()\n",
    "state_model = Model_signals_refStates_fixedBeta(\n",
    "    num_signals = generator.num_signals,\n",
    "    num_references = generator.num_references,\n",
    "    num_states = generator.num_states,\n",
    "    hidden = 32,\n",
    "    dropout = 0.2,\n",
    "    fixed_signalP  = p)\n",
    "state_model.to(device)\n",
    "optimizer = pyro.optim.Adam({\"lr\": learning_rate})\n",
    "svi = SVI(state_model.model, state_model.guide, optimizer, loss=TraceMeanField_ELBO())\n",
    "num_batches = int(math.ceil(m.shape[0] / batch_size))\n",
    "\n",
    "bar = trange(num_epochs)\n",
    "for epoch in bar:\n",
    "    running_loss = 0.0\n",
    "    for i in range(num_batches):\n",
    "        batch_m = m[i * batch_size:(i+1) * batch_size, :]\n",
    "        batch_r = r[i * batch_size:(i+1) * batch_size, :, :]\n",
    "        loss = svi.step(batch_m, batch_r)\n",
    "        running_loss += loss / batch_m.size(0)\n",
    "        \n",
    "    bar.set_postfix(epoch_loss='{:.2e}'.format(running_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "201d4194",
   "metadata": {},
   "outputs": [],
   "source": [
    "logpi_loc, logpi_scale = state_model.encoder(m, r)\n",
    "Cpi = F.softmax(logpi_loc, -1)\n",
    "signal_param, ref_param = state_model.decoder(Cpi) # vector of probabilities. \n",
    "re_m = pyro.sample('re_m', dist.Bernoulli(signal_param).to_event(1))\n",
    "re_r = pyro.sample('re_r', dist.Multinomial(1, ref_param).to_event(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d7470c65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(29997)\n",
      "tensor(960000)\n"
     ]
    }
   ],
   "source": [
    "ratio_m_CR, ratio_r_CR = state_model.get_percentage_correct_reconstruct(m,r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "272cd726",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "520000.0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(1000000 - torch.tensor(960000) /2).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63a594a4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
