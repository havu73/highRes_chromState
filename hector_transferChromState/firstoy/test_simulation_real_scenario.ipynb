{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "67fe203e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyro\n",
    "import pyro.distributions as dist\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import pyro\n",
    "import pyro.distributions as dist\n",
    "import torch.nn.functional as F\n",
    "from pyro.infer import SVI, TraceMeanField_ELBO\n",
    "from tqdm import trange\n",
    "import math\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bec1d299",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "M: # regions\n",
    "N: # bins per region\n",
    "L: # signals (signals)\n",
    "alpha: params of dirichlet prior over reference epigenomics\n",
    "beta: ref --> sample state categorical distribution\n",
    "p: state --> signal bernoulli distribution \n",
    "r: reference state at each bin. one-hot encoding, matrix size : #bins * #ref * #states\n",
    "theta: the mixture probabilities of reference ethetagenome\n",
    "'''\n",
    "class real_simulation:\n",
    "    def __init__(self):\n",
    "        self.num_bins = 10000\n",
    "        self.num_signals = 5\n",
    "        self.num_obs_signals = 3\n",
    "        self.num_references = 15\n",
    "        self.num_groups = 5\n",
    "        self.num_ref_per_groups = int(self.num_references/self.num_groups)\n",
    "        self.num_states = 8\n",
    "        self.num_const_states = 5\n",
    "        self.num_ctSpec_states = 3\n",
    "        self.state_vary_rate = 0.03\n",
    "        self.high_w = 10\n",
    "        self.sample = None\n",
    "        self.params = None #self.set_params()\n",
    "        \n",
    "    def generate_param_p(self):\n",
    "        '''\n",
    "        M1 --> 5: H3K4me3, H3K27ac, DNase, H3K4me1 (TSS like), RepressiveM\n",
    "        S0: H3K4me3, constitutive\n",
    "        S1: quiescent, constitutive\n",
    "        S2: RepressiveM, constitutive\n",
    "        S3: DNase, const\n",
    "        S4: Dnase + K4me1, const\n",
    "        S5: K27ac, ct-spec\n",
    "        S6: K27ac + RepressiveM, ct-spec\n",
    "        S7: poised TSS, K4me3 + repressiveM, ct-spec\n",
    "        '''\n",
    "        p = torch.zeros((self.num_states, self.num_signals)) - self.high_w\n",
    "        p[0,0] = self.high_w # K4me3\n",
    "        p[2,4] = self.high_w # RepressiveM\n",
    "        p[3,2] = self.high_w # Dnase\n",
    "        p[4,2] = self.high_w # Dnase\n",
    "        p[4,3] = self.high_w # K4me1\n",
    "        p[5,1] = self.high_w # K27ac\n",
    "        p[6,1] = self.high_w # K27ac\n",
    "        p[6,4] = self.high_w # RepressiveM\n",
    "        p[7,0] = self.high_w # K4me3\n",
    "        p[7,4] = self.high_w # RepressiveM\n",
    "        p = torch.sigmoid(p) # each row is a probability distribution over marks, sigmoid to convert to [0,1]\n",
    "        return p\n",
    "\n",
    "    # generate a state assignment tensor\n",
    "    # shape is (num_bins, num_references)\n",
    "    def generate_ref_states(self):\n",
    "        group_r = torch.ones(self.num_bins, self.num_groups)\n",
    "        const_ratio = 0.05 * self.num_const_states # 5 constitutive states\n",
    "        const_nBins = int(self.num_bins * const_ratio)\n",
    "        ctSpec_ratio = 0.05 * self.num_groups * self.num_ctSpec_states # 3 ct-spec states\n",
    "        ctSpec_nBins = self.num_bins - const_nBins\n",
    "        # sample constitutive position\n",
    "        const_bins = np.random.choice(self.num_bins, const_nBins, replace = False)\n",
    "        numBins_per_state = int(0.05 * self.num_bins)\n",
    "        for i in range(self.num_const_states): \n",
    "            # first num_const_states are const_states\n",
    "            bins_indices = torch.tensor(const_bins[i*numBins_per_state:(i+1)*numBins_per_state]).type(torch.LongTensor)\n",
    "            group_r[bins_indices,:] = i\n",
    "        # sample ctSpec positions\n",
    "        ctSpec_bins = torch.tensor(np.setdiff1d(np.arange(self.num_bins), const_bins)).type(torch.LongTensor)\n",
    "        num_ctSpec_bins_per_group = self.num_ref_per_groups*numBins_per_state\n",
    "        for i in range(self.num_groups):\n",
    "            indices_for_group = np.random.choice(ctSpec_bins, num_ctSpec_bins_per_group, replace = False)\n",
    "            ctSpec_bins = torch.tensor(np.setdiff1d(ctSpec_bins, indices_for_group)).type(torch.LongTensor)\n",
    "            # ctSpec_bins will shrink after each iteration to assign bins to each group\n",
    "            for j in range(self.num_ctSpec_states):\n",
    "                # after all constitutive states have been filled, \n",
    "                # we shall fill the ctSpec states\n",
    "                stateI = self.num_const_states + j\n",
    "                indices_for_state = np.random.choice(indices_for_group, numBins_per_state, replace = False)\n",
    "                group_r[indices_for_state, i] = stateI # ct_spec state in this group\n",
    "                indices_for_group = torch.tensor(np.setdiff1d(indices_for_group, indices_for_state)).type(torch.LongTensor)\n",
    "                # indices_for_group will shrink after each iteration to assign bins to state withing the group\n",
    "                # quiescent state in other groups by default because of how group_r is initiated\n",
    "        # now randomly introduce some noise by varying states among references of the same group, to be implemented\n",
    "        # create a new dimension: num_ref_per_group --> num_ref_per_group, num_bins, num_group: --> values: state\n",
    "        r = group_r.unsqueeze(0).repeat(self.num_ref_per_groups,1,1) \n",
    "        # unsqueeze(0) to add 1 dimension to the front\n",
    "        # repeat to repeat along each each dimension\n",
    "        return r.long() # convert to long because they will be used as index later\n",
    "    \n",
    "    # set parameters of the data generator\n",
    "    def set_params(self):\n",
    "        # parameters of the dirichlet over references\n",
    "        # same one for every region\n",
    "        # very high probability that generated sample looks like\n",
    "        # reference 0\n",
    "        # shape is (num_references,)\n",
    "        alpha = torch.ones(self.num_groups)\n",
    "        alpha[0] = self.high_w # we assume group 1 has the highest prob of being similar to sample of interest\n",
    "        \n",
    "        # parameters of bernoulli distribution for each signal\n",
    "        # for each state\n",
    "        # shape is (num_states, num_signals)\n",
    "        p = self.generate_param_p()\n",
    "        \n",
    "        # an indicator matrix along genome of the state for \n",
    "        # each refenrece\n",
    "        # shape is (num_ref_per_groups, num_bins, num_group, num_states)\n",
    "        ref_states_indicator = F.one_hot(self.generate_ref_states(), self.num_states)\n",
    "        # count of num_ref in each group that are annotated as each state\n",
    "        # shape is (num_bins, num_groups, num_states)\n",
    "        ref_states_count = ref_states_indicator.sum(axis = 0) \n",
    "        params = {\n",
    "            'alpha': alpha,\n",
    "            'p': p,\n",
    "            'ref_states_indicator': ref_states_indicator,\n",
    "            'ref_states_count' : ref_states_count\n",
    "        }\n",
    "        self.params = params\n",
    "        return params\n",
    "    \n",
    "    # collapse a prob vector over references to a prob vector over states\n",
    "    # takes the cross product of prob vector theta and reference state indicator matrix r\n",
    "    # shapes:\n",
    "    #  theta: (None, num_groups)\n",
    "    #  r: (None, num_groups, num_states)\n",
    "    #  out: (None, num_states)\n",
    "    def collapse_theta(self, theta, r=None):\n",
    "        if r is None:\n",
    "            assert self.params is not None\n",
    "            r = self.params['ref_states_count']\n",
    "        r = r.float()\n",
    "        collapsed_theta = torch.zeros(theta.shape[0], r.shape[2]) # bins, states\n",
    "        for i in range(theta.shape[0]):\n",
    "            collapsed_theta[i,:] = torch.matmul(r[i,:,:].T, theta[i,:])\n",
    "        collapsed_theta = collapsed_theta / float(self.num_ref_per_groups) \n",
    "        # the above line is added for the case of state count\n",
    "        # instead of state indicators\n",
    "        return collapsed_theta\n",
    "    \n",
    "    def generate_sample(self):\n",
    "        if self.params is None:\n",
    "            self.set_params()\n",
    "            \n",
    "        r = self.params['ref_states_count']\n",
    "                \n",
    "        # generate reference distribution for each region\n",
    "        with pyro.plate('bins', self.num_bins):\n",
    "            # theta is shape (num_regions, num_references)\n",
    "            theta = pyro.sample('theta', dist.Dirichlet(self.params['alpha']))\n",
    "            # collapse the reference distribution for each bin to a \n",
    "            # state distribution \n",
    "            collapsed_theta = self.collapse_theta(theta, r)\n",
    "        state = pyro.sample('state', dist.Categorical(collapsed_theta).to_event(1)) # 1D tensor of state indices\n",
    "        signal_params = self.params['p'][state,:] # each row shows the probabilities of signals given the selected state at the genomic bin\n",
    "        m = pyro.sample('m', dist.Bernoulli(signal_params).to_event(1))\n",
    "\n",
    "        result = {\n",
    "            'theta': theta,\n",
    "            'state': state,\n",
    "            'm': m\n",
    "        }\n",
    "        self.sample = result\n",
    "        return self.sample\n",
    "    \n",
    "\n",
    "    def get_sampled_collapsed_theta(self):\n",
    "        if self.sample is None:\n",
    "            self.generate_sample()\n",
    "        theta = self.sample['theta']\n",
    "        return self.collapse_theta(theta)\n",
    "\n",
    "    def save_collapsed_theta(self, output_fn):\n",
    "        collapsed_theta = self.get_sampled_collapsed_theta()\n",
    "        collapsed_theta = pd.DataFrame(collapsed_theta.numpy())\n",
    "        collapsed_theta['state'] = self.sample['state'].numpy()\n",
    "        collapsed_theta.to_csv(output_fn, header = True, index = False, sep = '\\t')\n",
    "        return \n",
    "\n",
    "    def get_sampled_signals(self):\n",
    "        if self.sample is None:\n",
    "            self.generate_sample()\n",
    "        return self.sample['m']\n",
    "    \n",
    "    def get_sampled_theta(self):\n",
    "        if self.sample is None:\n",
    "            self.generate_sample()\n",
    "        return self.sample['theta']\n",
    "    \n",
    "    def get_signal_parms(self):\n",
    "        if self.sample is None:\n",
    "            self.generate_sample()\n",
    "        collapsed_theta = self.get_sampled_collapsed_theta()\n",
    "        state = pyro.sample('state', dist.Categorical(collapsed_theta).to_event(1)) # 1D tensor of state indices\n",
    "        signal_params = self.params['p'][state,:]\n",
    "        return signal_params\n",
    "    \n",
    "    def get_state(self):\n",
    "        if self.sample is None:\n",
    "            self.generate_sample()\n",
    "        return self.sample['state']\n",
    "    \n",
    "    def get_ref_state_indicators(self):\n",
    "        if self.params is None:\n",
    "            self.set_params()\n",
    "        return self.params['ref_states_indicator']\n",
    "    \n",
    "    def get_ref_state_counts(self):\n",
    "        if self.params is None:\n",
    "            self.set_params()\n",
    "        return self.params['ref_states_count']\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "edbc972e",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 0\n",
    "torch.manual_seed(seed)\n",
    "pyro.set_rng_seed(seed)\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "generator = real_simulation()\n",
    "\n",
    "raw_m = generator.get_sampled_signals()\n",
    "m = raw_m[:,:3] # obs data should be only the first 3 marks\n",
    "r = generator.get_ref_state_counts().float()\n",
    "collapsed_theta = generator.get_sampled_collapsed_theta()\n",
    "state = generator.get_state()\n",
    "generator.save_collapsed_theta('trial_collapsed_theta.txt.gz')\n",
    "theta = generator.get_sampled_theta()\n",
    "signal_params = generator.get_signal_parms()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90ce431e",
   "metadata": {},
   "outputs": [],
   "source": [
    "toy = real_simulation()\n",
    "r = toy.generate_ref_states()\n",
    "# now let's do some testing that the function generate_ref_states is doing exactly what I want it to\n",
    "# Check # of bins in constitutive\n",
    "for i in range(5,8): # first 5 constitutive states\n",
    "    print(i)\n",
    "    num_val_per_row = torch.sum(r==i, 1)\n",
    "    uniq_count = torch.unique(num_val_per_row, return_counts = True)\n",
    "    print(uniq_count)\n",
    "# Check # bins where the states are different across the sample (ct_spec)\n",
    "# They they are different, make sure they are of the same group (3 samples, 5 samples apart)\n",
    "indices = ((r==7).nonzero(as_tuple= True))\n",
    "print(indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "05eefa78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "m: obs. signals at each position\n",
      "torch.Size([10000, 3])\n",
      "tensor([[0., 0., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.]])\n",
      "r: reference epigenome state indicator at each position\n",
      "torch.Size([10000, 5, 8])\n",
      "tensor([[[0., 3., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 3., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 3., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 3.],\n",
      "         [0., 3., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0.,  ..., 0., 3., 0.],\n",
      "         [0., 3., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 3., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 3., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 3., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 3., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 3., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 3., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 3.],\n",
      "         [0., 3., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[0., 0., 0.,  ..., 0., 3., 0.],\n",
      "         [0., 3., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 3., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 3., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 3., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 3., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 3., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 3., 0., 0.],\n",
      "         [0., 3., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 3., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 3.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 3.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 3.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 3.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 3.,  ..., 0., 0., 0.]]])\n",
      "collapsed_theta: state assignment at each position\n",
      "torch.Size([10000, 8])\n",
      "tensor([[0.0000, 0.9967, 0.0000,  ..., 0.0000, 0.0000, 0.0033],\n",
      "        [0.0000, 0.2946, 0.0000,  ..., 0.0000, 0.7054, 0.0000],\n",
      "        [0.0000, 0.8663, 0.0000,  ..., 0.0000, 0.0000, 0.1337],\n",
      "        ...,\n",
      "        [0.0000, 0.4338, 0.0000,  ..., 0.0000, 0.5662, 0.0000],\n",
      "        [0.0000, 0.9417, 0.0000,  ..., 0.0583, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 1.0000,  ..., 0.0000, 0.0000, 0.0000]])\n",
      "theta: the reference mixture at each position\n",
      "torch.Size([10000, 5])\n",
      "tensor([[0.9469, 0.0281, 0.0071, 0.0033, 0.0145],\n",
      "        [0.7054, 0.0153, 0.1691, 0.0252, 0.0850],\n",
      "        [0.6253, 0.1556, 0.0481, 0.1337, 0.0374],\n",
      "        ...,\n",
      "        [0.5662, 0.1027, 0.0882, 0.0997, 0.1432],\n",
      "        [0.7044, 0.1112, 0.0583, 0.0214, 0.1047],\n",
      "        [0.6508, 0.0425, 0.0427, 0.0954, 0.1685]])\n",
      "signal_params: bernoulli dist. params generating signal at each position\n",
      "torch.Size([10000, 5])\n",
      "p\n",
      "tensor([[9.9995e-01, 4.5398e-05, 4.5398e-05, 4.5398e-05, 4.5398e-05],\n",
      "        [4.5398e-05, 4.5398e-05, 4.5398e-05, 4.5398e-05, 4.5398e-05],\n",
      "        [4.5398e-05, 4.5398e-05, 4.5398e-05, 4.5398e-05, 9.9995e-01],\n",
      "        [4.5398e-05, 4.5398e-05, 9.9995e-01, 4.5398e-05, 4.5398e-05],\n",
      "        [4.5398e-05, 4.5398e-05, 9.9995e-01, 9.9995e-01, 4.5398e-05],\n",
      "        [4.5398e-05, 9.9995e-01, 4.5398e-05, 4.5398e-05, 4.5398e-05],\n",
      "        [4.5398e-05, 9.9995e-01, 4.5398e-05, 4.5398e-05, 9.9995e-01],\n",
      "        [9.9995e-01, 4.5398e-05, 4.5398e-05, 4.5398e-05, 9.9995e-01]])\n",
      "torch.Size([8, 5])\n"
     ]
    }
   ],
   "source": [
    "print('m: obs. signals at each position')\n",
    "print(m.shape)\n",
    "print(m)\n",
    "print('r: reference epigenome state indicator at each position')\n",
    "print(r.shape)\n",
    "print(r)\n",
    "print('collapsed_theta: state assignment at each position')\n",
    "print(collapsed_theta.shape)\n",
    "print(collapsed_theta)\n",
    "print('theta: the reference mixture at each position')\n",
    "print(theta.shape)\n",
    "print(theta)\n",
    "print('signal_params: bernoulli dist. params generating signal at each position')\n",
    "print(signal_params.shape)\n",
    "print('p')\n",
    "raw_p = generator.params['p']\n",
    "print (raw_p)\n",
    "print (raw_p.shape)\n",
    "p = raw_p[:,:3] # p given to model only include parameters of the observed marks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdc6137b",
   "metadata": {},
   "outputs": [],
   "source": [
    "t = r[:1,:,:]\n",
    "print(t)\n",
    "t2 = t.reshape(t.shape[0], -1)\n",
    "print(t2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f25613f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, num_signals, num_states, num_groups, hidden_sig, hidden_ref, hidden_comb, dropout):\n",
    "        super().__init__()\n",
    "        self.drop = nn.Dropout(dropout)\n",
    "        self.fc1 = nn.Linear(num_signals, hidden_sig)\n",
    "        self.fc2 = nn.Linear(num_states*num_groups, hidden_ref)\n",
    "        self.fc_comb = nn.Linear(hidden_sig+hidden_ref, hidden_comb)\n",
    "        self.fcmu = nn.Linear(hidden_comb, num_states)\n",
    "        self.fclv = nn.Linear(hidden_comb, num_states)\n",
    "\n",
    "    def forward(self, m, r):\n",
    "        r = r.reshape(r.shape[0], -1) # flatten from bins,groups,states --> bins, groups*states\n",
    "        h1 = F.softplus(self.fc1(m)) # hidden_sig\n",
    "        h2 = F.softplus(self.fc2(r)) # hidden_ref\n",
    "        h2 = self.drop(h2)\n",
    "        h = torch.cat((h1, h2), 1) # concat to increase # columns # hidden_sig + hidden_ref\n",
    "        h = F.softplus(self.fc_comb(h)) # hidden_comb\n",
    "        logpi_loc = self.fcmu(h) # num_states\n",
    "        logpi_logvar = self.fclv(h) # num_states\n",
    "        logpi_scale = (0.5 * logpi_logvar).exp()\n",
    "        return logpi_loc, logpi_scale\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, num_states, num_signals, num_groups, hidden_sig, hidden_ref, hidden_comb, dropout, fixed_signalP):\n",
    "        super().__init__()\n",
    "        self.num_states = num_states\n",
    "        self.num_signals = num_signals\n",
    "        self.num_groups = num_groups\n",
    "        self.drop = nn.Dropout(dropout)\n",
    "        self.fixed_signalP = fixed_signalP\n",
    "        self.fcih = nn.Linear(num_states, hidden_comb) # input (state probabilities) --> hidden\n",
    "        self.fchh_sig = nn.Linear(hidden_comb, hidden_sig) # hiddent --> hidden\n",
    "        self.fchs_sig = nn.Linear(hidden_sig, self.num_signals) # hidden --> signals\n",
    "        self.fchh_ref = nn.Linear(hidden_comb, hidden_ref) # hidden --> hidden_ref\n",
    "        self.fchr_ref = nn.Linear(hidden_ref, self.num_groups * self.num_states)\n",
    "\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        # takes in the values of collapsed pi: probabilities of state \n",
    "        # assignments at each positions, and then apply a linear trans\n",
    "        # to get the probabilities of observing signals at each position\n",
    "        # --> vector size #signals\n",
    "        # used as parameters for bernoulli dist. to get obs. signals\n",
    "        # create multiple layers\n",
    "        # inputs: bins, state probabilities\n",
    "        # h: bins, hidden \n",
    "        # signal_param: bins, signals\n",
    "        # ref_param: bins, num_groups, num_states\n",
    "        h = F.softplus(self.fcih(inputs)) # states --> hidden element vector\n",
    "        h = self.drop(h)\n",
    "        signal_param = torch.sigmoid(torch.matmul(inputs, self.fixed_signalP)) # hidden_sig --> marks\n",
    "        h_ref = F.softplus(self.fchh_ref(h)) # hidden_comb --> hidden_ref\n",
    "        ref_param = torch.sigmoid(self.fchr_ref(h_ref))\n",
    "        return signal_param, ref_param\n",
    "\n",
    "class Model_three(nn.Module):\n",
    "    def __init__(self, num_signals, num_groups, num_ref_per_groups, num_states, hidden_sig, hidden_ref, hidden_comb, dropout, fixed_signalP):\n",
    "        super().__init__()\n",
    "        self.num_signals = num_signals\n",
    "        self.num_groups = num_groups\n",
    "        self.num_ref_per_groups = num_ref_per_groups\n",
    "        self.num_states = num_states\n",
    "        self.hidden_sig = hidden_sig\n",
    "        self.hidden_ref = hidden_ref\n",
    "        self.hidden_comb = hidden_comb\n",
    "        self.fixed_signalP = fixed_signalP\n",
    "        self.dropout = dropout\n",
    "        self.encoder = Encoder(num_signals, num_states, num_groups, hidden_sig, hidden_ref, hidden_comb, dropout)\n",
    "        self.decoder = Decoder(num_states, num_signals, num_groups, hidden_sig, hidden_ref, hidden_comb, dropout, fixed_signalP)\n",
    "\n",
    "    # shapes: \n",
    "    #  m: (bins x signals) signal matrix\n",
    "    #  r: (bins x reference x state) indicator matrix\n",
    "    def model(self, m, r):\n",
    "        # flatten out the r indicator matrix\n",
    "        pyro.module(\"decoder\", self.decoder)\n",
    "        with pyro.plate('bins', m.shape[0]):\n",
    "            logCpi_loc = m.new_zeros((m.shape[0], self.num_states))\n",
    "            logCpi_scale = m.new_ones((m.shape[0], self.num_states))\n",
    "            logCpi = pyro.sample('log_collapsedPi', dist.Normal(logCpi_loc, logCpi_scale).to_event(1))\n",
    "            Cpi = F.softmax(logCpi, -1) \n",
    "            # the softmax function should be used here because Cpi is lognormal\n",
    "            signal_param, ref_param = self.decoder(Cpi) # vector of probabilities. \n",
    "            ref_param = ref_param.reshape((ref_param.shape[0], self.num_groups, self.num_states)) \n",
    "            # hidden --> num_ref*num_states\n",
    "            ref_param = F.normalize(ref_param, p = 1.0, dim = 2, eps = 1e-6) # row normalize, sum over states per ref is 1\n",
    "            # signal_param: bins, signals\n",
    "            # ref_param: bins, references, states\n",
    "            # first num_signals elements: bernoulli params\n",
    "            # each of the following num_states elements: multinomial params\n",
    "            # for the state segmentation in a reference    \n",
    "            pyro.sample('m', dist.Bernoulli(signal_param).to_event(1), obs=m)\n",
    "            # plate across references\n",
    "            with pyro.plate('refs', self.num_groups):\n",
    "                pyro.sample('r', dist.Multinomial(self.num_ref_per_groups, ref_param).to_event(1), obs = r)\n",
    "                # multinomial for non-homogeneous total_counts are not supported yet\n",
    "\n",
    "    def guide(self, m, r):\n",
    "        pyro.module(\"encoder\", self.encoder)\n",
    "        with pyro.plate('bins', m.shape[0]):\n",
    "            logpi_loc, logpi_scale = self.encoder(m, r)\n",
    "            try:\n",
    "                logpi = pyro.sample('log_collapsedPi', dist.Normal(logpi_loc, logpi_scale).to_event(1))\n",
    "            except:\n",
    "                torch.save(ref_param, 'problematic_logpi_loc.txt')\n",
    "                exit(1)\n",
    "\n",
    "    def predict_state_assignment(self, m, r):\n",
    "        logpi_loc, logpi_scale = self.encoder(m, r)\n",
    "        Cpi = F.softmax(logpi_loc, -1)\n",
    "        return(Cpi)\n",
    "\n",
    "    def write_predicted_state_assignment(self, m, r, output_fn):\n",
    "        Cpi = self.predict_state_assignment(m, r)\n",
    "        df = pd.DataFrame(Cpi.detach().numpy())\n",
    "        df['max_state'] = df.idxmax(axis = 1)\n",
    "        df.to_csv(output_fn, header = True, index = False, sep = '\\t', compression = 'gzip')\n",
    "        return\n",
    "\n",
    "    def generate_reconstructed_data(self, m, r):\n",
    "        logpi_loc, logpi_scale = self.encoder(m, r)\n",
    "        Cpi = F.softmax(logpi_loc, -1)\n",
    "        signal_param, ref_param = self.decoder(Cpi) # vector of probabilities. \n",
    "        ref_param = ref_param.reshape((ref_param.shape[0], self.num_groups, self.num_states)) \n",
    "        ref_param = F.normalize(ref_param, p = 1.0, dim = 2, eps = 1e-6)\n",
    "        re_m = pyro.sample('re_m', dist.Bernoulli(signal_param).to_event(1))\n",
    "        re_r = pyro.sample('re_r', dist.Multinomial(1, ref_param).to_event(1))\n",
    "        return(re_m, re_r)\n",
    "\n",
    "\n",
    "    def get_percentage_correct_reconstruct(self, m, r):\n",
    "        # m and r can be different from the m and r used in training\n",
    "        re_m, re_r = self.generate_reconstructed_data(m,r)\n",
    "        # re_r: bins, groups, states --> counts\n",
    "        total_m_entries = re_m.shape[0] * re_m.shape[1]\n",
    "        signals_CR = (re_m==m).sum() # correct reconstruct entries of signals\n",
    "        total_r_entries = re_r.shape[0] * self.num_groups\n",
    "        # for each reference at each position, if the state assignment is different between re_r and r, there are 2 out of num_states entries that are different between re_r and r\n",
    "        wrong_r = torch.abs(re_r - r)/(2.0 * self.num_ref_per_groups) # # bins and average references per group that re_r got wrong\n",
    "        r_CR = total_r_entries - wrong_r.sum()\n",
    "        ratio_m_CR = (signals_CR / total_m_entries).item()\n",
    "        ratio_r_CR = (r_CR / total_r_entries).item()\n",
    "        return ratio_m_CR, ratio_r_CR\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2026fceb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1000/1000 [06:17<00:00,  2.65it/s, epoch_loss=1.24e+03]\n"
     ]
    }
   ],
   "source": [
    "batch_size = 200\n",
    "learning_rate = 1e-3\n",
    "num_epochs = 1000\n",
    "pyro.clear_param_store()\n",
    "state_model = Model_three(\n",
    "    num_signals = generator.num_signals - 2,\n",
    "    num_groups = generator.num_groups,\n",
    "    num_ref_per_groups = generator.num_ref_per_groups,\n",
    "    num_states = generator.num_states,\n",
    "    hidden_sig = 1,\n",
    "    hidden_ref = 20,\n",
    "    hidden_comb = 21,\n",
    "    dropout = 0.2, \n",
    "    fixed_signalP = p)\n",
    "state_model.to(device)\n",
    "optimizer = pyro.optim.Adam({\"lr\": learning_rate})\n",
    "svi = SVI(state_model.model, state_model.guide, optimizer, loss=TraceMeanField_ELBO())\n",
    "num_batches = int(math.ceil(m.shape[0] / batch_size))\n",
    "\n",
    "bar = trange(num_epochs)\n",
    "for epoch in bar:\n",
    "    running_loss = 0.0\n",
    "    for i in range(num_batches):\n",
    "        batch_m = m[i * batch_size:(i+1) * batch_size, :]\n",
    "        batch_r = r[i * batch_size:(i+1) * batch_size, :, :]\n",
    "        loss = svi.step(batch_m, batch_r)\n",
    "        running_loss += loss / batch_m.size(0)\n",
    "        \n",
    "    bar.set_postfix(epoch_loss='{:.2e}'.format(running_loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5c1871d",
   "metadata": {},
   "source": [
    "## Model with reference states and fixed beta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39b718f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "\tdef __init__(self, num_signals, num_states, num_groups, hidden, dropout):\n",
    "\t\tsuper().__init__()\n",
    "\t\tself.drop = nn.Dropout(dropout)\n",
    "\t\tinput_dim = num_signals + num_states * num_groups\n",
    "\t\tself.fc1 = nn.Linear(input_dim, hidden)\n",
    "\t\tself.fc2 = nn.Linear(hidden, hidden)\n",
    "\t\tself.fcmu = nn.Linear(hidden, num_states)\n",
    "\t\tself.fclv = nn.Linear(hidden, num_states)\n",
    "\n",
    "\tdef forward(self, m, r):\n",
    "\t\tinputs = torch.cat((m, r.reshape(r.shape[0], -1)), 1)\n",
    "\t\th = F.softplus(self.fc1(inputs))\n",
    "\t\th = F.softplus(self.fc2(h))\n",
    "\t\th = self.drop(h)\n",
    "\t\tlogpi_loc = (self.fcmu(h))\n",
    "\t\tlogpi_logvar = self.fclv(h)\n",
    "\t\tlogpi_scale = (0.5 * logpi_logvar).exp()\n",
    "\t\treturn logpi_loc, logpi_scale\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "\tdef __init__(self, num_states, num_signals, num_groups, hidden, dropout, fixed_signalP):\n",
    "\t\tsuper().__init__()\n",
    "\t\tself.num_states = num_states\n",
    "\t\tself.num_signals = num_signals\n",
    "\t\tself.num_groups = num_groups\n",
    "\t\tself.fixed_signalP = fixed_signalP\n",
    "\t\tself.drop = nn.Dropout(dropout)\n",
    "\t\tself.fcih = nn.Linear(num_states, hidden) # input (state probabilities) --> hidden\n",
    "\t\tself.fchh = nn.Linear(hidden, hidden) # hiddent --> hidden\n",
    "\t\tself.fchs = nn.Linear(hidden, self.num_signals) # hidden --> signals\n",
    "\t\tself.fchr = nn.Linear(hidden, self.num_groups * self.num_states) # hidden --> states in references\n",
    "\n",
    "\n",
    "\n",
    "\tdef forward(self, inputs):\n",
    "\t\t# takes in the values of collapsed pi: probabilities of state \n",
    "\t\t# assignments at each positions, and then apply a linear trans\n",
    "\t\t# to get the probabilities of observing signals at each position\n",
    "\t\t# --> vector size #signals\n",
    "\t\t# used as parameters for bernoulli dist. to get obs. signals\n",
    "\t\t# create multiple layers\n",
    "\t\t# inputs: bins, state probabilities\n",
    "\t\t# h: bins, hidden \n",
    "\t\t# signal_param: bins, signals\n",
    "\t\t# ref_param: bins, num_groups, num_states\n",
    "\t\th = F.softplus(self.fcih(inputs)) # --> hidden element vector\n",
    "\t\th = F.softplus(self.fchh(h)) # --> hidden element vector\n",
    "\t\th = self.drop(h)\n",
    "\t\tsignal_param = torch.sigmoid(torch.matmul(inputs, self.fixed_signalP)) # hidden --> marks\n",
    "\t\tref_param = torch.sigmoid(self.fchr(h)).reshape((h.shape[0], self.num_groups, self.num_states)) \n",
    "\t\t# hidden --> num_ref*num_states\n",
    "\t\tref_param = F.normalize(ref_param, p = 1.0, dim = 2) # row normalize, sum over states per ref is 1\n",
    "\t\treturn signal_param, ref_param\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "class Model_signals_refStates_fixedBeta(nn.Module):\n",
    "\tdef __init__(self, num_signals, num_groups, num_ref_per_groups, num_states, hidden, dropout, fixed_signalP):\n",
    "\t\tsuper().__init__()\n",
    "\t\tself.num_signals = num_signals\n",
    "\t\tself.num_groups = num_groups\n",
    "\t\tself.num_ref_per_groups = num_ref_per_groups\n",
    "\t\tself.num_states = num_states\n",
    "\t\tself.hidden = hidden\n",
    "\t\tself.dropout = dropout\n",
    "\t\tself.fixed_signalP = fixed_signalP\n",
    "\t\tself.encoder = Encoder(num_signals, num_states, num_groups, hidden, dropout)\n",
    "\t\tself.decoder = Decoder(num_states, num_signals, num_groups, hidden, dropout, fixed_signalP)\n",
    "\n",
    "\t# shapes: \n",
    "\t#  m: (bins x signals) signal matrix\n",
    "\t#  r: (bins x reference x state) indicator matrix\n",
    "\tdef model(self, m, r):\n",
    "\t\t# flatten out the r indicator matrix\n",
    "\t\tpyro.module(\"decoder\", self.decoder)\n",
    "\t\twith pyro.plate('bins', m.shape[0]):\n",
    "\t\t\tlogCpi_loc = m.new_zeros((m.shape[0], self.num_states))\n",
    "\t\t\tlogCpi_scale = m.new_ones((m.shape[0], self.num_states))\n",
    "\t\t\tlogCpi = pyro.sample('log_collapsedPi', dist.Normal(logCpi_loc, logCpi_scale).to_event(1))\n",
    "\t\t\tCpi = F.softmax(logCpi, -1) \n",
    "\t\t\t# the softmax function should be used here because Cpi is lognormal\n",
    "\t\t\tsignal_param, ref_param = self.decoder(Cpi) # vector of probabilities. \n",
    "\t\t\t# signal_param: bins, signals\n",
    "\t\t\t# ref_param: bins, references, states\n",
    "\t\t\t# first num_signals elements: bernoulli params\n",
    "\t\t\t# each of the following num_states elements: multinomial params\n",
    "\t\t\t# for the state segmentation in a reference \n",
    "\t\t\tpyro.sample('m', dist.Bernoulli(signal_param).to_event(1), obs=m)\n",
    "\t\t\t# plate across references\n",
    "\t\t\twith pyro.plate('refs', self.num_groups):\n",
    "\t\t\t\tpyro.sample('r', dist.Multinomial(self.num_ref_per_groups, ref_param).to_event(1), obs = r)\n",
    "\n",
    "\tdef guide(self, m, r):\n",
    "\t\tpyro.module(\"encoder\", self.encoder)\n",
    "\t\twith pyro.plate('bins', m.shape[0]):\n",
    "\t\t\tlogpi_loc, logpi_scale = self.encoder(m, r)\n",
    "\t\t\tlogpi = pyro.sample('log_collapsedPi', dist.Normal(logpi_loc, logpi_scale).to_event(1))\n",
    "\n",
    "\tdef predict_state_assignment(self, m, r):\n",
    "\t\tlogpi_loc, logpi_scale = self.encoder(m, r)\n",
    "\t\tCpi = F.softmax(logpi_loc, -1)\n",
    "\t\treturn(Cpi)\n",
    "\n",
    "\n",
    "\tdef generate_reconstructed_data(self, m, r):\n",
    "\t\tlogpi_loc, logpi_scale = self.encoder(m, r)\n",
    "\t\tCpi = F.softmax(logpi_loc, -1)\n",
    "\t\tsignal_param, ref_param = self.decoder(Cpi) # vector of probabilities. \n",
    "\t\tre_m = pyro.sample('re_m', dist.Bernoulli(signal_param).to_event(1))\n",
    "\t\tre_r = pyro.sample('re_r', dist.Multinomial(1, ref_param).to_event(1))\n",
    "\t\treturn(re_m, re_r)\n",
    "\n",
    "\n",
    "\tdef get_percentage_correct_reconstruct(self, m, r):\n",
    "\t\t# m and r can be different from the m and r used in training\n",
    "\t\tre_m, re_r = self.generate_reconstructed_data(m,r)\n",
    "\t\ttotal_m_entries = re_m.shape[0] * re_m.shape[1]\n",
    "\t\tsignals_CR = (re_m==m).sum() # correct reconstruct entries of signals\n",
    "\t\ttotal_r_entries = re_r.shape[0] * self.num_groups\n",
    "\t\t# for each reference at each position, if the state assignment is different between re_r and r, there are 2 out of num_states entries that are different between re_r and r\n",
    "\t\twrong_r = ((re_r.shape[0] * re_r.shape[1] * re_r.shape[2]) - (re_r==r).sum()) / 2 # wrong reconstruct entries of reference states\n",
    "\t\tr_CR = total_r_entries - wrong_r\n",
    "\t\tratio_m_CR = (signals_CR / total_m_entries).item()\n",
    "\t\tratio_r_CR = (r_CR / total_r_entries).item()\n",
    "\t\treturn ratio_m_CR, ratio_r_CR\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a743d71f",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 200\n",
    "learning_rate = 1e-3\n",
    "num_epochs = 1000\n",
    "pyro.clear_param_store()\n",
    "state_model = Model_signals_refStates_fixedBeta(\n",
    "    num_signals = generator.num_signals - 2,\n",
    "    num_groups = generator.num_groups,\n",
    "    num_ref_per_groups = generator.num_ref_per_groups,\n",
    "    num_states = generator.num_states,\n",
    "    hidden = 32,\n",
    "    dropout = 0.2,\n",
    "    fixed_signalP  = p)\n",
    "state_model.to(device)\n",
    "optimizer = pyro.optim.Adam({\"lr\": learning_rate})\n",
    "svi = SVI(state_model.model, state_model.guide, optimizer, loss=TraceMeanField_ELBO())\n",
    "num_batches = int(math.ceil(m.shape[0] / batch_size))\n",
    "bar = trange(num_epochs)\n",
    "for epoch in bar:\n",
    "    running_loss = 0.0\n",
    "    for i in range(num_batches):\n",
    "        batch_m = m[i * batch_size:(i+1) * batch_size, :]\n",
    "        batch_r = r[i * batch_size:(i+1) * batch_size, :, :]\n",
    "        loss = svi.step(batch_m, batch_r)\n",
    "        running_loss += loss / batch_m.size(0)\n",
    "        \n",
    "    bar.set_postfix(epoch_loss='{:.2e}'.format(running_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5768724a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ratio_m_CR: 0.45320001244544983\n",
      "ratio_r_CR: 0.6280733346939087\n"
     ]
    }
   ],
   "source": [
    "ratio_m_CR, ratio_r_CR = state_model.get_percentage_correct_reconstruct(m,r)\n",
    "print('ratio_m_CR: {}'.format(ratio_m_CR))\n",
    "print('ratio_r_CR: {}'.format(ratio_r_CR))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3c9ee83f",
   "metadata": {},
   "outputs": [],
   "source": [
    "Cpi = state_model.predict_state_assignment(m, r)\n",
    "pred_df = pd.DataFrame(Cpi.detach().numpy())\n",
    "pred_df['max_state'] = pred_df.idxmax(axis =1)\n",
    "pred_df.columns = list(map(lambda x: 'pred|{}'.format(x), pred_df.columns))\n",
    "collapsed_theta = generator.get_sampled_collapsed_theta()\n",
    "collapsed_theta = pd.DataFrame(collapsed_theta.numpy())\n",
    "collapsed_theta['max_state'] = collapsed_theta.idxmax(axis = 1)\n",
    "collapsed_theta.columns = list(map(lambda x: 'true|{}'.format(x), collapsed_theta.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "82e5df84",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pred_df.merge(collapsed_theta, left_index = True, right_index = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6be686cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[['true|max_state', 'pred|max_state']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6a736c71",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "colnames = ['true|max_state', 'pred|max_state']\n",
    "t = df[colnames]\n",
    "t = t.groupby(colnames).size().to_frame(name = 'size').reset_index()\n",
    "t = t.pivot(colnames[0], colnames[1], 'size')\n",
    "t = t.div(t.sum(axis = 1), axis = 0) # row normalize\n",
    "sns.heatmap(t, cbar=True, linewidths=2,vmax=1, vmin=0, square=True, cmap='Blues')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e46b5fd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(collapsed_theta.columns)\n",
    "collapsed_theta['true|max_state'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2c57886",
   "metadata": {},
   "outputs": [],
   "source": [
    "t = torch.rand((2,3,4)) # bins, refs, states\n",
    "t = F.softmax(t, -1)\n",
    "t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd2bec77",
   "metadata": {},
   "outputs": [],
   "source": [
    "pyro.sample('r', dist.Multinomial(3, t).to_event(1))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
